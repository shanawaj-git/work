{"version":3,"file":"index.cjs.js","sources":["../src/stages/publish/helpers.ts","../src/stages/generate/helpers.ts","../src/stages/generate/mkDocsPatchers.ts","../src/stages/generate/techdocs.ts","../src/stages/generate/generators.ts","../src/helpers.ts","../src/stages/prepare/dir.ts","../src/stages/prepare/url.ts","../src/stages/prepare/preparers.ts","../src/stages/publish/awsS3.ts","../src/stages/publish/azureBlobStorage.ts","../src/stages/publish/migrations/GoogleMigration.ts","../src/stages/publish/googleStorage.ts","../src/stages/publish/local.ts","../src/stages/publish/openStackSwift.ts","../src/stages/publish/publish.ts"],"sourcesContent":["/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Entity, DEFAULT_NAMESPACE } from '@backstage/catalog-model';\nimport mime from 'mime-types';\nimport path from 'path';\nimport createLimiter from 'p-limit';\nimport recursiveReadDir from 'recursive-readdir';\n\n/**\n * Helper to get the expected content-type for a given file extension. Also\n * takes XSS mitigation into account.\n */\nconst getContentTypeForExtension = (ext: string): string => {\n  const defaultContentType = 'text/plain; charset=utf-8';\n\n  // Prevent sanitization bypass by preventing browsers from directly rendering\n  // the contents of untrusted files.\n  if (ext.match(/htm|xml|svg/i)) {\n    return defaultContentType;\n  }\n\n  return mime.contentType(ext) || defaultContentType;\n};\n\nexport type responseHeadersType = {\n  'Content-Type': string;\n};\n\n/**\n * Some files need special headers to be used correctly by the frontend. This function\n * generates headers in the response to those file requests.\n * @param fileExtension - .html, .css, .js, .png etc.\n */\nexport const getHeadersForFileExtension = (\n  fileExtension: string,\n): responseHeadersType => {\n  return {\n    'Content-Type': getContentTypeForExtension(fileExtension),\n  };\n};\n\n/**\n * Recursively traverse all the sub-directories of a path and return\n * a list of absolute paths of all the files. e.g. tree command in Unix\n *\n * @example\n *\n * /User/username/my_dir\n *     dirA\n *     |   subDirA\n *     |   |   file1\n *     EmptyDir\n *     dirB\n *     |   file2\n *     file3\n *\n * getFileListRecursively('/Users/username/myDir')\n * // returns\n * [\n *   '/User/username/my_dir/dirA/subDirA/file1',\n *   '/User/username/my_dir/dirB/file2',\n *   '/User/username/my_dir/file3'\n * ]\n * @param rootDirPath - Absolute path to the root directory.\n */\nexport const getFileTreeRecursively = async (\n  rootDirPath: string,\n): Promise<string[]> => {\n  // Iterate on all the files in the directory and its sub-directories\n  const fileList = await recursiveReadDir(rootDirPath).catch(error => {\n    throw new Error(`Failed to read template directory: ${error.message}`);\n  });\n  return fileList;\n};\n\n/**\n * Takes a posix path and returns a lower-cased version of entity's triplet\n * with the remaining path in posix.\n *\n * Path must not include a starting slash.\n *\n * @example\n * lowerCaseEntityTriplet('default/Component/backstage')\n * // return default/component/backstage\n */\nexport const lowerCaseEntityTriplet = (posixPath: string): string => {\n  const [namespace, kind, name, ...rest] = posixPath.split(path.posix.sep);\n  const lowerNamespace = namespace.toLowerCase();\n  const lowerKind = kind.toLowerCase();\n  const lowerName = name.toLowerCase();\n  return [lowerNamespace, lowerKind, lowerName, ...rest].join(path.posix.sep);\n};\n\n/**\n * Takes either a win32 or posix path and returns a lower-cased version of entity's triplet\n * with the remaining path in posix.\n *\n * Starting slashes will be trimmed.\n *\n * Throws an error if the path does not appear to be an entity triplet.\n *\n * @example\n * lowerCaseEntityTripletInStoragePath('/default/Component/backstage/file.txt')\n * // return default/component/backstage/file.txt\n */\nexport const lowerCaseEntityTripletInStoragePath = (\n  originalPath: string,\n): string => {\n  let posixPath = originalPath;\n  if (originalPath.includes(path.win32.sep)) {\n    posixPath = originalPath.split(path.win32.sep).join(path.posix.sep);\n  }\n\n  // remove leading slash\n  const parts = posixPath.split(path.posix.sep);\n  if (parts[0] === '') {\n    parts.shift();\n  }\n\n  // check if all parts of the entity exist (name, namespace, kind) plus filename\n  if (parts.length <= 3) {\n    throw new Error(\n      `Encountered file unmanaged by TechDocs ${originalPath}. Skipping.`,\n    );\n  }\n\n  return lowerCaseEntityTriplet(parts.join(path.posix.sep));\n};\n\n/**\n * Take a posix path and return a path without leading and trailing\n * separators\n *\n * @example\n * normalizeExternalStorageRootPath('/backstage-data/techdocs/')\n * // return backstage-data/techdocs\n */\nexport const normalizeExternalStorageRootPath = (posixPath: string): string => {\n  // remove leading slash\n  let normalizedPath = posixPath;\n  if (posixPath.startsWith(path.posix.sep)) {\n    normalizedPath = posixPath.slice(1);\n  }\n\n  // remove trailing slash\n  if (normalizedPath.endsWith(path.posix.sep)) {\n    normalizedPath = normalizedPath.slice(0, normalizedPath.length - 1);\n  }\n\n  return normalizedPath;\n};\n\n// Only returns the files that existed previously and are not present anymore.\nexport const getStaleFiles = (\n  newFiles: string[],\n  oldFiles: string[],\n): string[] => {\n  const staleFiles = new Set(oldFiles);\n  newFiles.forEach(newFile => {\n    staleFiles.delete(newFile);\n  });\n  return Array.from(staleFiles);\n};\n\n// Compose actual filename on remote bucket including entity information\nexport const getCloudPathForLocalPath = (\n  entity: Entity,\n  localPath = '',\n  useLegacyPathCasing = false,\n  externalStorageRootPath = '',\n): string => {\n  // Convert destination file path to a POSIX path for uploading.\n  // GCS expects / as path separator and relativeFilePath will contain \\\\ on Windows.\n  // https://cloud.google.com/storage/docs/gsutil/addlhelp/HowSubdirectoriesWork\n  const relativeFilePathPosix = localPath.split(path.sep).join(path.posix.sep);\n\n  // The / delimiter is intentional since it represents the cloud storage and not the local file system.\n  const entityRootDir = `${entity.metadata?.namespace ?? DEFAULT_NAMESPACE}/${\n    entity.kind\n  }/${entity.metadata.name}`;\n\n  const relativeFilePathTriplet = `${entityRootDir}/${relativeFilePathPosix}`;\n\n  const destination = useLegacyPathCasing\n    ? relativeFilePathTriplet\n    : lowerCaseEntityTriplet(relativeFilePathTriplet);\n\n  // Again, the / delimiter is intentional, as it represents remote storage.\n  const destinationWithRoot = [\n    // The extra filter prevents unintended double slashes and prefixes.\n    ...externalStorageRootPath.split(path.posix.sep).filter(s => s !== ''),\n    destination,\n  ].join('/');\n\n  return destinationWithRoot; // Remote storage file relative path\n};\n\n// Perform rate limited generic operations by passing a function and a list of arguments\nexport const bulkStorageOperation = async <T>(\n  operation: (arg: T) => Promise<unknown>,\n  args: T[],\n  { concurrencyLimit } = { concurrencyLimit: 25 },\n) => {\n  const limiter = createLimiter(concurrencyLimit);\n  await Promise.all(args.map(arg => limiter(operation, arg)));\n};\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { isChildPath } from '@backstage/backend-common';\nimport { Entity } from '@backstage/catalog-model';\nimport { assertError, ForwardedError } from '@backstage/errors';\nimport { ScmIntegrationRegistry } from '@backstage/integration';\nimport { SpawnOptionsWithoutStdio, spawn } from 'child_process';\nimport fs from 'fs-extra';\nimport gitUrlParse from 'git-url-parse';\nimport yaml, { DEFAULT_SCHEMA, Type } from 'js-yaml';\nimport path, { resolve as resolvePath } from 'path';\nimport { PassThrough, Writable } from 'stream';\nimport { Logger } from 'winston';\nimport { ParsedLocationAnnotation } from '../../helpers';\nimport { SupportedGeneratorKey } from './types';\nimport { getFileTreeRecursively } from '../publish/helpers';\n\n// TODO: Implement proper support for more generators.\nexport function getGeneratorKey(entity: Entity): SupportedGeneratorKey {\n  if (!entity) {\n    throw new Error('No entity provided');\n  }\n\n  return 'techdocs';\n}\n\nexport type RunCommandOptions = {\n  /** command to run */\n  command: string;\n  /** arguments to pass the command */\n  args: string[];\n  /** options to pass to spawn */\n  options: SpawnOptionsWithoutStdio;\n  /** stream to capture stdout and stderr output */\n  logStream?: Writable;\n};\n\n/**\n * Run a command in a sub-process, normally a shell command.\n */\nexport const runCommand = async ({\n  command,\n  args,\n  options,\n  logStream = new PassThrough(),\n}: RunCommandOptions) => {\n  await new Promise<void>((resolve, reject) => {\n    const process = spawn(command, args, options);\n\n    process.stdout.on('data', stream => {\n      logStream.write(stream);\n    });\n\n    process.stderr.on('data', stream => {\n      logStream.write(stream);\n    });\n\n    process.on('error', error => {\n      return reject(error);\n    });\n\n    process.on('close', code => {\n      if (code !== 0) {\n        return reject(`Command ${command} failed, exit code: ${code}`);\n      }\n      return resolve();\n    });\n  });\n};\n\n/**\n * Return the source url for MkDocs based on the backstage.io/techdocs-ref annotation.\n * Depending on the type of target, it can either return a repo_url, an edit_uri, both, or none.\n *\n * @param parsedLocationAnnotation - Object with location url and type\n * @param scmIntegrations - the scmIntegration to do url transformations\n * @param docsFolder - the configured docs folder in the mkdocs.yml (defaults to 'docs')\n * @returns the settings for the mkdocs.yml\n */\nexport const getRepoUrlFromLocationAnnotation = (\n  parsedLocationAnnotation: ParsedLocationAnnotation,\n  scmIntegrations: ScmIntegrationRegistry,\n  docsFolder: string = 'docs',\n): { repo_url?: string; edit_uri?: string } => {\n  const { type: locationType, target } = parsedLocationAnnotation;\n\n  if (locationType === 'url') {\n    const integration = scmIntegrations.byUrl(target);\n\n    // We only support it for github and gitlab for now as the edit_uri\n    // is not properly supported for others yet.\n    if (integration && ['github', 'gitlab'].includes(integration.type)) {\n      // handle the case where a user manually writes url:https://github.com/backstage/backstage i.e. without /blob/...\n      const { filepathtype } = gitUrlParse(target);\n      if (filepathtype === '') {\n        return { repo_url: target };\n      }\n\n      const sourceFolder = integration.resolveUrl({\n        url: `./${docsFolder}`,\n        base: target,\n      });\n      return { edit_uri: integration.resolveEditUrl(sourceFolder) };\n    }\n  }\n\n  return {};\n};\n\nclass UnknownTag {\n  constructor(public readonly data: any, public readonly type?: string) {}\n}\n\nexport const MKDOCS_SCHEMA = DEFAULT_SCHEMA.extend([\n  new Type('', {\n    kind: 'scalar',\n    multi: true,\n    representName: o => (o as UnknownTag).type,\n    represent: o => (o as UnknownTag).data ?? '',\n    instanceOf: UnknownTag,\n    construct: (data: string, type?: string) => new UnknownTag(data, type),\n  }),\n]);\n\n/**\n * Finds and loads the contents of either an mkdocs.yml or mkdocs.yaml file,\n * depending on which is present (MkDocs supports both as of v1.2.2).\n *\n * @param inputDir - base dir to be searched for either an mkdocs.yml or\n *   mkdocs.yaml file.\n */\nexport const getMkdocsYml = async (\n  inputDir: string,\n): Promise<{ path: string; content: string }> => {\n  let mkdocsYmlPath: string;\n  let mkdocsYmlFileString: string;\n  try {\n    mkdocsYmlPath = path.join(inputDir, 'mkdocs.yaml');\n    mkdocsYmlFileString = await fs.readFile(mkdocsYmlPath, 'utf8');\n  } catch {\n    try {\n      mkdocsYmlPath = path.join(inputDir, 'mkdocs.yml');\n      mkdocsYmlFileString = await fs.readFile(mkdocsYmlPath, 'utf8');\n    } catch (error) {\n      throw new ForwardedError(\n        'Could not read MkDocs YAML config file mkdocs.yml or mkdocs.yaml for validation',\n        error,\n      );\n    }\n  }\n\n  return {\n    path: mkdocsYmlPath,\n    content: mkdocsYmlFileString,\n  };\n};\n\n/**\n * Validating mkdocs config file for incorrect/insecure values\n * Throws on invalid configs\n *\n * @param inputDir - base dir to be used as a docs_dir path validity check\n * @param mkdocsYmlFileString - The string contents of the loaded\n *   mkdocs.yml or equivalent of a docs site\n * @returns the parsed docs_dir or undefined\n */\nexport const validateMkdocsYaml = async (\n  inputDir: string,\n  mkdocsYmlFileString: string,\n): Promise<string | undefined> => {\n  const mkdocsYml = yaml.load(mkdocsYmlFileString, {\n    schema: MKDOCS_SCHEMA,\n  });\n\n  if (mkdocsYml === null || typeof mkdocsYml !== 'object') {\n    return undefined;\n  }\n\n  const parsedMkdocsYml: Record<string, any> = mkdocsYml;\n  if (\n    parsedMkdocsYml.docs_dir &&\n    !isChildPath(inputDir, resolvePath(inputDir, parsedMkdocsYml.docs_dir))\n  ) {\n    throw new Error(\n      `docs_dir configuration value in mkdocs can't be an absolute directory or start with ../ for security reasons.\n       Use relative paths instead which are resolved relative to your mkdocs.yml file location.`,\n    );\n  }\n  return parsedMkdocsYml.docs_dir;\n};\n\n/**\n * Update docs/index.md file before TechDocs generator uses it to generate docs site,\n * falling back to docs/README.md or README.md in case a default docs/index.md\n * is not provided.\n */\nexport const patchIndexPreBuild = async ({\n  inputDir,\n  logger,\n  docsDir = 'docs',\n}: {\n  inputDir: string;\n  logger: Logger;\n  docsDir?: string;\n}) => {\n  const docsPath = path.join(inputDir, docsDir);\n  const indexMdPath = path.join(docsPath, 'index.md');\n\n  if (await fs.pathExists(indexMdPath)) {\n    return;\n  }\n  logger.warn(`${path.join(docsDir, 'index.md')} not found.`);\n  const fallbacks = [\n    path.join(docsPath, 'README.md'),\n    path.join(docsPath, 'readme.md'),\n    path.join(inputDir, 'README.md'),\n    path.join(inputDir, 'readme.md'),\n  ];\n\n  await fs.ensureDir(docsPath);\n  for (const filePath of fallbacks) {\n    try {\n      await fs.copyFile(filePath, indexMdPath);\n      return;\n    } catch (error) {\n      logger.warn(`${path.relative(inputDir, filePath)} not found.`);\n    }\n  }\n\n  logger.warn(\n    `Could not find any techdocs' index file. Please make sure at least one of ${[\n      indexMdPath,\n      ...fallbacks,\n    ].join(' ')} exists.`,\n  );\n};\n\n/**\n * Create or update the techdocs_metadata.json. Values initialized/updated are:\n * - The build_timestamp (now)\n * - The list of files generated\n *\n * @param techdocsMetadataPath - File path to techdocs_metadata.json\n */\nexport const createOrUpdateMetadata = async (\n  techdocsMetadataPath: string,\n  logger: Logger,\n): Promise<void> => {\n  const techdocsMetadataDir = techdocsMetadataPath\n    .split(path.sep)\n    .slice(0, -1)\n    .join(path.sep);\n  // check if file exists, create if it does not.\n  try {\n    await fs.access(techdocsMetadataPath, fs.constants.F_OK);\n  } catch (err) {\n    // Bootstrap file with empty JSON\n    await fs.writeJson(techdocsMetadataPath, JSON.parse('{}'));\n  }\n  // check if valid Json\n  let json;\n  try {\n    json = await fs.readJson(techdocsMetadataPath);\n  } catch (err) {\n    assertError(err);\n    const message = `Invalid JSON at ${techdocsMetadataPath} with error ${err.message}`;\n    logger.error(message);\n    throw new Error(message);\n  }\n\n  json.build_timestamp = Date.now();\n\n  // Get and write generated files to the metadata JSON. Each file string is in\n  // a form appropriate for invalidating the associated object from cache.\n  try {\n    json.files = (await getFileTreeRecursively(techdocsMetadataDir)).map(file =>\n      file.replace(`${techdocsMetadataDir}${path.sep}`, ''),\n    );\n  } catch (err) {\n    assertError(err);\n    json.files = [];\n    logger.warn(`Unable to add files list to metadata: ${err.message}`);\n  }\n\n  await fs.writeJson(techdocsMetadataPath, json);\n  return;\n};\n\n/**\n * Update the techdocs_metadata.json to add etag of the prepared tree (e.g. commit SHA or actual Etag of the resource).\n * This is helpful to check if a TechDocs site in storage has gone outdated, without maintaining an in-memory build info\n * per Backstage instance.\n *\n * @param techdocsMetadataPath - File path to techdocs_metadata.json\n * @param etag - The ETag to use\n */\nexport const storeEtagMetadata = async (\n  techdocsMetadataPath: string,\n  etag: string,\n): Promise<void> => {\n  const json = await fs.readJson(techdocsMetadataPath);\n  json.etag = etag;\n  await fs.writeJson(techdocsMetadataPath, json);\n};\n","/*\n * Copyright 2022 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Logger } from 'winston';\nimport fs from 'fs-extra';\nimport yaml from 'js-yaml';\nimport { ParsedLocationAnnotation } from '../../helpers';\nimport { getRepoUrlFromLocationAnnotation, MKDOCS_SCHEMA } from './helpers';\nimport { assertError } from '@backstage/errors';\nimport { ScmIntegrationRegistry } from '@backstage/integration';\n\ntype MkDocsObject = {\n  plugins?: string[];\n  docs_dir: string;\n  repo_url?: string;\n  edit_uri?: string;\n};\n\nconst patchMkdocsFile = async (\n  mkdocsYmlPath: string,\n  logger: Logger,\n  updateAction: (mkdocsYml: MkDocsObject) => boolean,\n) => {\n  // We only want to override the mkdocs.yml if it has actually changed. This is relevant if\n  // used with a 'dir' location on the file system as this would permanently update the file.\n  let didEdit = false;\n\n  let mkdocsYmlFileString;\n  try {\n    mkdocsYmlFileString = await fs.readFile(mkdocsYmlPath, 'utf8');\n  } catch (error) {\n    assertError(error);\n    logger.warn(\n      `Could not read MkDocs YAML config file ${mkdocsYmlPath} before running the generator: ${error.message}`,\n    );\n    return;\n  }\n\n  let mkdocsYml: any;\n  try {\n    mkdocsYml = yaml.load(mkdocsYmlFileString, { schema: MKDOCS_SCHEMA });\n\n    // mkdocsYml should be an object type after successful parsing.\n    // But based on its type definition, it can also be a string or undefined, which we don't want.\n    if (typeof mkdocsYml === 'string' || typeof mkdocsYml === 'undefined') {\n      throw new Error('Bad YAML format.');\n    }\n  } catch (error) {\n    assertError(error);\n    logger.warn(\n      `Error in parsing YAML at ${mkdocsYmlPath} before running the generator. ${error.message}`,\n    );\n    return;\n  }\n\n  didEdit = updateAction(mkdocsYml);\n\n  try {\n    if (didEdit) {\n      await fs.writeFile(\n        mkdocsYmlPath,\n        yaml.dump(mkdocsYml, { schema: MKDOCS_SCHEMA }),\n        'utf8',\n      );\n    }\n  } catch (error) {\n    assertError(error);\n    logger.warn(\n      `Could not write to ${mkdocsYmlPath} after updating it before running the generator. ${error.message}`,\n    );\n    return;\n  }\n};\n\n/**\n * Update the mkdocs.yml file before TechDocs generator uses it to generate docs site.\n *\n * List of tasks:\n * - Add repo_url or edit_uri if it does not exists\n * If mkdocs.yml has a repo_url, the generated docs site gets an Edit button on the pages by default.\n * If repo_url is missing in mkdocs.yml, we will use techdocs annotation of the entity to possibly get\n * the repository URL.\n *\n * This function will not throw an error since this is not critical to the whole TechDocs pipeline.\n * Instead it will log warnings if there are any errors in reading, parsing or writing YAML.\n *\n * @param mkdocsYmlPath - Absolute path to mkdocs.yml or equivalent of a docs site\n * @param logger - A logger instance\n * @param parsedLocationAnnotation - Object with location url and type\n * @param scmIntegrations - the scmIntegration to do url transformations\n */\nexport const patchMkdocsYmlPreBuild = async (\n  mkdocsYmlPath: string,\n  logger: Logger,\n  parsedLocationAnnotation: ParsedLocationAnnotation,\n  scmIntegrations: ScmIntegrationRegistry,\n) => {\n  await patchMkdocsFile(mkdocsYmlPath, logger, mkdocsYml => {\n    if (!('repo_url' in mkdocsYml) && !('edit_uri' in mkdocsYml)) {\n      // Add edit_uri and/or repo_url to mkdocs.yml if it is missing.\n      // This will enable the Page edit button generated by MkDocs.\n      // If the either has been set, keep the original value\n      const result = getRepoUrlFromLocationAnnotation(\n        parsedLocationAnnotation,\n        scmIntegrations,\n        mkdocsYml.docs_dir,\n      );\n\n      if (result.repo_url || result.edit_uri) {\n        mkdocsYml.repo_url = result.repo_url;\n        mkdocsYml.edit_uri = result.edit_uri;\n\n        logger.info(\n          `Set ${JSON.stringify(\n            result,\n          )}. You can disable this feature by manually setting 'repo_url' or 'edit_uri' according to the MkDocs documentation at https://www.mkdocs.org/user-guide/configuration/#repo_url`,\n        );\n        return true;\n      }\n    }\n    return false;\n  });\n};\n\n/**\n * Update the mkdocs.yml file before TechDocs generator uses it to generate docs site.\n *\n * List of tasks:\n * - Add techdocs-core plugin to mkdocs file if it doesn't exist\n *\n * This function will not throw an error since this is not critical to the whole TechDocs pipeline.\n * Instead it will log warnings if there are any errors in reading, parsing or writing YAML.\n *\n * @param mkdocsYmlPath - Absolute path to mkdocs.yml or equivalent of a docs site\n * @param logger - A logger instance\n */\nexport const pathMkdocsYmlWithTechdocsPlugin = async (\n  mkdocsYmlPath: string,\n  logger: Logger,\n) => {\n  await patchMkdocsFile(mkdocsYmlPath, logger, mkdocsYml => {\n    // Modify mkdocs.yaml to contain the needed techdocs-core plugin if it is not there\n    if (!('plugins' in mkdocsYml)) {\n      mkdocsYml.plugins = ['techdocs-core'];\n      return true;\n    }\n\n    if (mkdocsYml.plugins && !mkdocsYml.plugins.includes('techdocs-core')) {\n      mkdocsYml.plugins.push('techdocs-core');\n      return true;\n    }\n    return false;\n  });\n};\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { ContainerRunner } from '@backstage/backend-common';\nimport { Config } from '@backstage/config';\nimport path from 'path';\nimport { Logger } from 'winston';\nimport {\n  ScmIntegrationRegistry,\n  ScmIntegrations,\n} from '@backstage/integration';\nimport {\n  createOrUpdateMetadata,\n  getMkdocsYml,\n  patchIndexPreBuild,\n  runCommand,\n  storeEtagMetadata,\n  validateMkdocsYaml,\n} from './helpers';\n\nimport {\n  patchMkdocsYmlPreBuild,\n  pathMkdocsYmlWithTechdocsPlugin,\n} from './mkDocsPatchers';\nimport {\n  GeneratorBase,\n  GeneratorConfig,\n  GeneratorOptions,\n  GeneratorRunInType,\n  GeneratorRunOptions,\n} from './types';\nimport { ForwardedError } from '@backstage/errors';\n\n/**\n * Generates documentation files\n * @public\n */\nexport class TechdocsGenerator implements GeneratorBase {\n  /**\n   * The default docker image (and version) used to generate content. Public\n   * and static so that techdocs-node consumers can use the same version.\n   */\n  public static readonly defaultDockerImage = 'spotify/techdocs:v0.3.7';\n  private readonly logger: Logger;\n  private readonly containerRunner: ContainerRunner;\n  private readonly options: GeneratorConfig;\n  private readonly scmIntegrations: ScmIntegrationRegistry;\n\n  /**\n   * Returns a instance of TechDocs generator\n   * @param config - A Backstage configuration\n   * @param options - Options to configure the generator\n   */\n  static fromConfig(config: Config, options: GeneratorOptions) {\n    const { containerRunner, logger } = options;\n    const scmIntegrations = ScmIntegrations.fromConfig(config);\n    return new TechdocsGenerator({\n      logger,\n      containerRunner,\n      config,\n      scmIntegrations,\n    });\n  }\n\n  constructor(options: {\n    logger: Logger;\n    containerRunner: ContainerRunner;\n    config: Config;\n    scmIntegrations: ScmIntegrationRegistry;\n  }) {\n    this.logger = options.logger;\n    this.options = readGeneratorConfig(options.config, options.logger);\n    this.containerRunner = options.containerRunner;\n    this.scmIntegrations = options.scmIntegrations;\n  }\n\n  /** {@inheritDoc GeneratorBase.run} */\n  public async run(options: GeneratorRunOptions): Promise<void> {\n    const {\n      inputDir,\n      outputDir,\n      parsedLocationAnnotation,\n      etag,\n      logger: childLogger,\n      logStream,\n    } = options;\n\n    // Do some updates to mkdocs.yml before generating docs e.g. adding repo_url\n    const { path: mkdocsYmlPath, content } = await getMkdocsYml(inputDir);\n\n    // validate the docs_dir first\n    const docsDir = await validateMkdocsYaml(inputDir, content);\n\n    if (parsedLocationAnnotation) {\n      await patchMkdocsYmlPreBuild(\n        mkdocsYmlPath,\n        childLogger,\n        parsedLocationAnnotation,\n        this.scmIntegrations,\n      );\n      await patchIndexPreBuild({ inputDir, logger: childLogger, docsDir });\n    }\n\n    if (!this.options.omitTechdocsCoreMkdocsPlugin) {\n      await pathMkdocsYmlWithTechdocsPlugin(mkdocsYmlPath, childLogger);\n    }\n\n    // Directories to bind on container\n    const mountDirs = {\n      [inputDir]: '/input',\n      [outputDir]: '/output',\n    };\n\n    try {\n      switch (this.options.runIn) {\n        case 'local':\n          await runCommand({\n            command: 'mkdocs',\n            args: ['build', '-d', outputDir, '-v'],\n            options: {\n              cwd: inputDir,\n            },\n            logStream,\n          });\n          childLogger.info(\n            `Successfully generated docs from ${inputDir} into ${outputDir} using local mkdocs`,\n          );\n          break;\n        case 'docker':\n          await this.containerRunner.runContainer({\n            imageName:\n              this.options.dockerImage ?? TechdocsGenerator.defaultDockerImage,\n            args: ['build', '-d', '/output'],\n            logStream,\n            mountDirs,\n            workingDir: '/input',\n            // Set the home directory inside the container as something that applications can\n            // write to, otherwise they will just fail trying to write to /\n            envVars: { HOME: '/tmp' },\n            pullImage: this.options.pullImage,\n          });\n          childLogger.info(\n            `Successfully generated docs from ${inputDir} into ${outputDir} using techdocs-container`,\n          );\n          break;\n        default:\n          throw new Error(\n            `Invalid config value \"${this.options.runIn}\" provided in 'techdocs.generators.techdocs'.`,\n          );\n      }\n    } catch (error) {\n      this.logger.debug(\n        `Failed to generate docs from ${inputDir} into ${outputDir}`,\n      );\n      throw new ForwardedError(\n        `Failed to generate docs from ${inputDir} into ${outputDir}`,\n        error,\n      );\n    }\n\n    /**\n     * Post Generate steps\n     */\n\n    // Add build timestamp and files to techdocs_metadata.json\n    // Creates techdocs_metadata.json if file does not exist.\n    await createOrUpdateMetadata(\n      path.join(outputDir, 'techdocs_metadata.json'),\n      childLogger,\n    );\n\n    // Add etag of the prepared tree to techdocs_metadata.json\n    // Assumes that the file already exists.\n    if (etag) {\n      await storeEtagMetadata(\n        path.join(outputDir, 'techdocs_metadata.json'),\n        etag,\n      );\n    }\n  }\n}\n\nexport function readGeneratorConfig(\n  config: Config,\n  logger: Logger,\n): GeneratorConfig {\n  const legacyGeneratorType = config.getOptionalString(\n    'techdocs.generators.techdocs',\n  ) as GeneratorRunInType;\n\n  if (legacyGeneratorType) {\n    logger.warn(\n      `The 'techdocs.generators.techdocs' configuration key is deprecated and will be removed in the future. Please use 'techdocs.generator' instead. ` +\n        `See here https://backstage.io/docs/features/techdocs/configuration`,\n    );\n  }\n\n  return {\n    runIn:\n      legacyGeneratorType ??\n      config.getOptionalString('techdocs.generator.runIn') ??\n      'docker',\n    dockerImage: config.getOptionalString('techdocs.generator.dockerImage'),\n    pullImage: config.getOptionalBoolean('techdocs.generator.pullImage'),\n    omitTechdocsCoreMkdocsPlugin: config.getOptionalBoolean(\n      'techdocs.generator.mkdocs.omitTechdocsCorePlugin',\n    ),\n  };\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { ContainerRunner } from '@backstage/backend-common';\nimport { Entity } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { Logger } from 'winston';\nimport { getGeneratorKey } from './helpers';\nimport { TechdocsGenerator } from './techdocs';\nimport {\n  GeneratorBase,\n  GeneratorBuilder,\n  SupportedGeneratorKey,\n} from './types';\n\n/**\n * Collection of docs generators\n * @public\n */\nexport class Generators implements GeneratorBuilder {\n  private generatorMap = new Map<SupportedGeneratorKey, GeneratorBase>();\n\n  /**\n   * Returns a generators instance containing a generator for TechDocs\n   * @param config - A Backstage configuration\n   * @param options - Options to configure the TechDocs generator\n   */\n  static async fromConfig(\n    config: Config,\n    options: { logger: Logger; containerRunner: ContainerRunner },\n  ): Promise<GeneratorBuilder> {\n    const generators = new Generators();\n\n    const techdocsGenerator = TechdocsGenerator.fromConfig(config, options);\n    generators.register('techdocs', techdocsGenerator);\n\n    return generators;\n  }\n\n  /**\n   * Register a generator in the generators collection\n   * @param generatorKey - Unique identifier for the generator\n   * @param generator - The generator instance to register\n   */\n  register(generatorKey: SupportedGeneratorKey, generator: GeneratorBase) {\n    this.generatorMap.set(generatorKey, generator);\n  }\n\n  /**\n   * Returns the generator for a given TechDocs entity\n   * @param entity - A TechDocs entity instance\n   */\n  get(entity: Entity): GeneratorBase {\n    const generatorKey = getGeneratorKey(entity);\n    const generator = this.generatorMap.get(generatorKey);\n\n    if (!generator) {\n      throw new Error(`No generator registered for entity: \"${generatorKey}\"`);\n    }\n\n    return generator;\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { resolveSafeChildPath, UrlReader } from '@backstage/backend-common';\nimport {\n  Entity,\n  getEntitySourceLocation,\n  parseLocationRef,\n} from '@backstage/catalog-model';\nimport { InputError } from '@backstage/errors';\nimport { ScmIntegrationRegistry } from '@backstage/integration';\nimport path from 'path';\nimport { Logger } from 'winston';\nimport { PreparerResponse, RemoteProtocol } from './stages/prepare/types';\n\n/**\n * Parsed location annotation\n * @public\n */\nexport type ParsedLocationAnnotation = {\n  type: RemoteProtocol;\n  target: string;\n};\n\n/**\n * Returns a parset locations annotation\n * @public\n * @param annotationName - The name of the annotation in the entity metadata\n * @param entity - A TechDocs entity instance\n */\nexport const parseReferenceAnnotation = (\n  annotationName: string,\n  entity: Entity,\n): ParsedLocationAnnotation => {\n  const annotation = entity.metadata.annotations?.[annotationName];\n  if (!annotation) {\n    throw new InputError(\n      `No location annotation provided in entity: ${entity.metadata.name}`,\n    );\n  }\n\n  const { type, target } = parseLocationRef(annotation);\n  return {\n    type: type as RemoteProtocol,\n    target,\n  };\n};\n\n/**\n * TechDocs references of type `dir` are relative the source location of the entity.\n * This function transforms relative references to absolute ones, based on the\n * location the entity was ingested from. If the entity was registered by a `url`\n * location, it returns a `url` location with a resolved target that points to the\n * targeted subfolder. If the entity was registered by a `file` location, it returns\n * an absolute `dir` location.\n * @public\n * @param entity - the entity with annotations\n * @param dirAnnotation - the parsed techdocs-ref annotation of type 'dir'\n * @param scmIntegrations - access to the scmIntegration to do url transformations\n * @throws if the entity doesn't specify a `dir` location or is ingested from an unsupported location.\n * @returns the transformed location with an absolute target.\n */\nexport const transformDirLocation = (\n  entity: Entity,\n  dirAnnotation: ParsedLocationAnnotation,\n  scmIntegrations: ScmIntegrationRegistry,\n): { type: 'dir' | 'url'; target: string } => {\n  const location = getEntitySourceLocation(entity);\n\n  switch (location.type) {\n    case 'url': {\n      const target = scmIntegrations.resolveUrl({\n        url: dirAnnotation.target,\n        base: location.target,\n      });\n\n      return {\n        type: 'url',\n        target,\n      };\n    }\n\n    case 'file': {\n      // only permit targets in the same folder as the target of the `file` location!\n      const target = resolveSafeChildPath(\n        path.dirname(location.target),\n        dirAnnotation.target,\n      );\n\n      return {\n        type: 'dir',\n        target,\n      };\n    }\n\n    default:\n      throw new InputError(`Unable to resolve location type ${location.type}`);\n  }\n};\n\n/**\n * Returns a entity reference based on the TechDocs annotation type\n * @public\n * @param entity - A TechDocs instance\n * @param scmIntegration - An implementation for  SCM integration API\n */\nexport const getLocationForEntity = (\n  entity: Entity,\n  scmIntegration: ScmIntegrationRegistry,\n): ParsedLocationAnnotation => {\n  const annotation = parseReferenceAnnotation(\n    'backstage.io/techdocs-ref',\n    entity,\n  );\n\n  switch (annotation.type) {\n    case 'url':\n      return annotation;\n    case 'dir':\n      return transformDirLocation(entity, annotation, scmIntegration);\n    default:\n      throw new Error(`Invalid reference annotation ${annotation.type}`);\n  }\n};\n\n/**\n * Returns a preparer response {@link PreparerResponse}\n * @public\n * @param reader - Read a tree of files from a repository\n * @param entity - A TechDocs entity instance\n * @param opts - Options for configuring the reader, e.g. logger, etag, etc.\n */\nexport const getDocFilesFromRepository = async (\n  reader: UrlReader,\n  entity: Entity,\n  opts?: { etag?: string; logger?: Logger },\n): Promise<PreparerResponse> => {\n  const { target } = parseReferenceAnnotation(\n    'backstage.io/techdocs-ref',\n    entity,\n  );\n\n  opts?.logger?.debug(`Reading files from ${target}`);\n  // readTree will throw NotModifiedError if etag has not changed.\n  const readTreeResponse = await reader.readTree(target, { etag: opts?.etag });\n  const preparedDir = await readTreeResponse.dir();\n\n  opts?.logger?.debug(`Tree downloaded and stored at ${preparedDir}`);\n\n  return {\n    preparedDir,\n    etag: readTreeResponse.etag,\n  };\n};\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { UrlReader } from '@backstage/backend-common';\nimport { Entity } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { InputError } from '@backstage/errors';\nimport {\n  ScmIntegrationRegistry,\n  ScmIntegrations,\n} from '@backstage/integration';\nimport { Logger } from 'winston';\nimport { parseReferenceAnnotation, transformDirLocation } from '../../helpers';\nimport {\n  PreparerBase,\n  PreparerConfig,\n  PreparerOptions,\n  PreparerResponse,\n} from './types';\n\n/**\n * Preparer used to retrieve documentation files from a local directory\n * @public\n */\nexport class DirectoryPreparer implements PreparerBase {\n  private readonly scmIntegrations: ScmIntegrationRegistry;\n  private readonly reader: UrlReader;\n\n  /**  @deprecated use static fromConfig method instead */\n  constructor(config: Config, _logger: Logger | null, reader: UrlReader) {\n    this.reader = reader;\n    this.scmIntegrations = ScmIntegrations.fromConfig(config);\n  }\n\n  /**\n   * Returns a directory preparer instance\n   * @param config - A backstage config\n   * @param options - A directory preparer options containing a logger and reader\n   */\n  static fromConfig(\n    config: Config,\n    { logger, reader }: PreparerConfig,\n  ): DirectoryPreparer {\n    return new DirectoryPreparer(config, logger, reader);\n  }\n\n  /** {@inheritDoc PreparerBase.prepare} */\n  async prepare(\n    entity: Entity,\n    options?: PreparerOptions,\n  ): Promise<PreparerResponse> {\n    const annotation = parseReferenceAnnotation(\n      'backstage.io/techdocs-ref',\n      entity,\n    );\n    const { type, target } = transformDirLocation(\n      entity,\n      annotation,\n      this.scmIntegrations,\n    );\n\n    switch (type) {\n      case 'url': {\n        options?.logger?.debug(`Reading files from ${target}`);\n        // the target is an absolute url since it has already been transformed\n        const response = await this.reader.readTree(target, {\n          etag: options?.etag,\n        });\n        const preparedDir = await response.dir();\n\n        options?.logger?.debug(`Tree downloaded and stored at ${preparedDir}`);\n\n        return {\n          preparedDir,\n          etag: response.etag,\n        };\n      }\n\n      case 'dir': {\n        return {\n          // the transformation already validated that the target is in a safe location\n          preparedDir: target,\n          // Instead of supporting caching on local sources, use techdocs-cli for local development and debugging.\n          etag: '',\n        };\n      }\n\n      default:\n        throw new InputError(`Unable to resolve location type ${type}`);\n    }\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { assertError } from '@backstage/errors';\nimport { UrlReader } from '@backstage/backend-common';\nimport { Entity } from '@backstage/catalog-model';\nimport { Logger } from 'winston';\nimport { getDocFilesFromRepository } from '../../helpers';\nimport {\n  PreparerBase,\n  PreparerConfig,\n  PreparerOptions,\n  PreparerResponse,\n} from './types';\n\n/**\n * Preparer used to retrieve documentation files from a remote repository\n * @public\n */\nexport class UrlPreparer implements PreparerBase {\n  private readonly logger: Logger;\n  private readonly reader: UrlReader;\n\n  /**  @deprecated use static fromConfig method instead */\n  constructor(reader: UrlReader, logger: Logger) {\n    this.logger = logger;\n    this.reader = reader;\n  }\n\n  /**\n   * Returns a directory preparer instance\n   * @param config - A URL preparer config containing the a logger and reader\n   */\n  static fromConfig({ reader, logger }: PreparerConfig): UrlPreparer {\n    return new UrlPreparer(reader, logger);\n  }\n\n  /** {@inheritDoc PreparerBase.prepare} */\n  async prepare(\n    entity: Entity,\n    options?: PreparerOptions,\n  ): Promise<PreparerResponse> {\n    try {\n      return await getDocFilesFromRepository(this.reader, entity, {\n        etag: options?.etag,\n        logger: this.logger,\n      });\n    } catch (error) {\n      assertError(error);\n      // NotModifiedError means that etag based cache is still valid.\n      if (error.name === 'NotModifiedError') {\n        this.logger.debug(`Cache is valid for etag ${options?.etag}`);\n      } else {\n        this.logger.debug(\n          `Unable to fetch files for building docs ${error.message}`,\n        );\n      }\n\n      throw error;\n    }\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Entity } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { parseReferenceAnnotation } from '../../helpers';\nimport { DirectoryPreparer } from './dir';\nimport { UrlPreparer } from './url';\nimport {\n  PreparerBase,\n  PreparerBuilder,\n  PreparerConfig,\n  RemoteProtocol,\n} from './types';\n\n/**\n * Collection of docs preparers (dir and url)\n * @public\n */\nexport class Preparers implements PreparerBuilder {\n  private preparerMap = new Map<RemoteProtocol, PreparerBase>();\n\n  /**\n   * Returns a generators instance containing a generator for TechDocs\n   * @public\n   * @param backstageConfig - A Backstage configuration\n   * @param preparerConfig - Options to configure preparers\n   */\n  static async fromConfig(\n    backstageConfig: Config,\n    { logger, reader }: PreparerConfig,\n  ): Promise<PreparerBuilder> {\n    const preparers = new Preparers();\n\n    const urlPreparer = new UrlPreparer(reader, logger);\n    preparers.register('url', urlPreparer);\n\n    /**\n     * Dir preparer is a syntactic sugar for users to define techdocs-ref annotation.\n     * When using dir preparer, the docs will be fetched using URL Reader.\n     */\n    const directoryPreparer = new DirectoryPreparer(\n      backstageConfig,\n      logger,\n      reader,\n    );\n    preparers.register('dir', directoryPreparer);\n\n    return preparers;\n  }\n\n  /**\n   * Register a preparer in the preparers collection\n   * @param protocol - url or dir to associate with preparer\n   * @param preparer - The preparer instance to set\n   */\n  register(protocol: RemoteProtocol, preparer: PreparerBase) {\n    this.preparerMap.set(protocol, preparer);\n  }\n\n  /**\n   * Returns the preparer for a given TechDocs entity\n   * @param entity - A TechDocs entity instance\n   * @returns\n   */\n  get(entity: Entity): PreparerBase {\n    const { type } = parseReferenceAnnotation(\n      'backstage.io/techdocs-ref',\n      entity,\n    );\n    const preparer = this.preparerMap.get(type);\n\n    if (!preparer) {\n      throw new Error(`No preparer registered for type: \"${type}\"`);\n    }\n\n    return preparer;\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Entity, CompoundEntityRef } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { assertError, ForwardedError } from '@backstage/errors';\nimport aws, { Credentials } from 'aws-sdk';\nimport { ListObjectsV2Output } from 'aws-sdk/clients/s3';\nimport { CredentialsOptions } from 'aws-sdk/lib/credentials';\nimport express from 'express';\nimport fs from 'fs-extra';\nimport JSON5 from 'json5';\nimport createLimiter from 'p-limit';\nimport path from 'path';\nimport { Readable } from 'stream';\nimport { Logger } from 'winston';\nimport {\n  bulkStorageOperation,\n  getCloudPathForLocalPath,\n  getFileTreeRecursively,\n  getHeadersForFileExtension,\n  getStaleFiles,\n  lowerCaseEntityTriplet,\n  lowerCaseEntityTripletInStoragePath,\n  normalizeExternalStorageRootPath,\n} from './helpers';\nimport {\n  PublisherBase,\n  PublishRequest,\n  PublishResponse,\n  ReadinessResponse,\n  TechDocsMetadata,\n} from './types';\n\nconst streamToBuffer = (stream: Readable): Promise<Buffer> => {\n  return new Promise((resolve, reject) => {\n    try {\n      const chunks: any[] = [];\n      stream.on('data', chunk => chunks.push(chunk));\n      stream.on('error', reject);\n      stream.on('end', () => resolve(Buffer.concat(chunks)));\n    } catch (e) {\n      throw new ForwardedError('Unable to parse the response data', e);\n    }\n  });\n};\n\nexport class AwsS3Publish implements PublisherBase {\n  private readonly storageClient: aws.S3;\n  private readonly bucketName: string;\n  private readonly legacyPathCasing: boolean;\n  private readonly logger: Logger;\n  private readonly bucketRootPath: string;\n  private readonly sse?: 'aws:kms' | 'AES256';\n\n  constructor(options: {\n    storageClient: aws.S3;\n    bucketName: string;\n    legacyPathCasing: boolean;\n    logger: Logger;\n    bucketRootPath: string;\n    sse?: 'aws:kms' | 'AES256';\n  }) {\n    this.storageClient = options.storageClient;\n    this.bucketName = options.bucketName;\n    this.legacyPathCasing = options.legacyPathCasing;\n    this.logger = options.logger;\n    this.bucketRootPath = options.bucketRootPath;\n    this.sse = options.sse;\n  }\n\n  static fromConfig(config: Config, logger: Logger): PublisherBase {\n    let bucketName = '';\n    try {\n      bucketName = config.getString('techdocs.publisher.awsS3.bucketName');\n    } catch (error) {\n      throw new Error(\n        \"Since techdocs.publisher.type is set to 'awsS3' in your app config, \" +\n          'techdocs.publisher.awsS3.bucketName is required.',\n      );\n    }\n\n    const bucketRootPath = normalizeExternalStorageRootPath(\n      config.getOptionalString('techdocs.publisher.awsS3.bucketRootPath') || '',\n    );\n\n    const sse = config.getOptionalString('techdocs.publisher.awsS3.sse') as\n      | 'aws:kms'\n      | 'AES256'\n      | undefined;\n\n    // Credentials is an optional config. If missing, the default ways of authenticating AWS SDK V2 will be used.\n    // 1. AWS environment variables\n    // https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/loading-node-credentials-environment.html\n    // 2. AWS shared credentials file at ~/.aws/credentials\n    // https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/loading-node-credentials-shared.html\n    // 3. IAM Roles for EC2\n    // https://docs.aws.amazon.com/sdk-for-javascript/v2/developer-guide/loading-node-credentials-iam.html\n    const credentialsConfig = config.getOptionalConfig(\n      'techdocs.publisher.awsS3.credentials',\n    );\n    const credentials = AwsS3Publish.buildCredentials(credentialsConfig);\n\n    // AWS Region is an optional config. If missing, default AWS env variable AWS_REGION\n    // or AWS shared credentials file at ~/.aws/credentials will be used.\n    const region = config.getOptionalString('techdocs.publisher.awsS3.region');\n\n    // AWS endpoint is an optional config. If missing, the default endpoint is built from\n    // the configured region.\n    const endpoint = config.getOptionalString(\n      'techdocs.publisher.awsS3.endpoint',\n    );\n\n    // AWS forcePathStyle is an optional config. If missing, it defaults to false. Needs to be enabled for cases\n    // where endpoint url points to locally hosted S3 compatible storage like Localstack\n    const s3ForcePathStyle = config.getOptionalBoolean(\n      'techdocs.publisher.awsS3.s3ForcePathStyle',\n    );\n\n    const storageClient = new aws.S3({\n      credentials,\n      ...(region && { region }),\n      ...(endpoint && { endpoint }),\n      ...(s3ForcePathStyle && { s3ForcePathStyle }),\n    });\n\n    const legacyPathCasing =\n      config.getOptionalBoolean(\n        'techdocs.legacyUseCaseSensitiveTripletPaths',\n      ) || false;\n\n    return new AwsS3Publish({\n      storageClient,\n      bucketName,\n      bucketRootPath,\n      legacyPathCasing,\n      logger,\n      sse,\n    });\n  }\n\n  private static buildCredentials(\n    config?: Config,\n  ): Credentials | CredentialsOptions | undefined {\n    if (!config) {\n      return undefined;\n    }\n\n    const accessKeyId = config.getOptionalString('accessKeyId');\n    const secretAccessKey = config.getOptionalString('secretAccessKey');\n    let explicitCredentials: Credentials | undefined;\n    if (accessKeyId && secretAccessKey) {\n      explicitCredentials = new Credentials({\n        accessKeyId,\n        secretAccessKey,\n      });\n    }\n\n    const roleArn = config.getOptionalString('roleArn');\n    if (roleArn) {\n      return new aws.ChainableTemporaryCredentials({\n        masterCredentials: explicitCredentials,\n        params: {\n          RoleSessionName: 'backstage-aws-techdocs-s3-publisher',\n          RoleArn: roleArn,\n        },\n      });\n    }\n\n    return explicitCredentials;\n  }\n\n  /**\n   * Check if the defined bucket exists. Being able to connect means the configuration is good\n   * and the storage client will work.\n   */\n  async getReadiness(): Promise<ReadinessResponse> {\n    try {\n      await this.storageClient\n        .headBucket({ Bucket: this.bucketName })\n        .promise();\n\n      this.logger.info(\n        `Successfully connected to the AWS S3 bucket ${this.bucketName}.`,\n      );\n\n      return { isAvailable: true };\n    } catch (error) {\n      this.logger.error(\n        `Could not retrieve metadata about the AWS S3 bucket ${this.bucketName}. ` +\n          'Make sure the bucket exists. Also make sure that authentication is setup either by ' +\n          'explicitly defining credentials and region in techdocs.publisher.awsS3 in app config or ' +\n          'by using environment variables. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage',\n      );\n      this.logger.error(`from AWS client library`, error);\n      return {\n        isAvailable: false,\n      };\n    }\n  }\n\n  /**\n   * Upload all the files from the generated `directory` to the S3 bucket.\n   * Directory structure used in the bucket is - entityNamespace/entityKind/entityName/index.html\n   */\n  async publish({\n    entity,\n    directory,\n  }: PublishRequest): Promise<PublishResponse> {\n    const objects: string[] = [];\n    const useLegacyPathCasing = this.legacyPathCasing;\n    const bucketRootPath = this.bucketRootPath;\n    const sse = this.sse;\n\n    // First, try to retrieve a list of all individual files currently existing\n    let existingFiles: string[] = [];\n    try {\n      const remoteFolder = getCloudPathForLocalPath(\n        entity,\n        undefined,\n        useLegacyPathCasing,\n        bucketRootPath,\n      );\n      existingFiles = await this.getAllObjectsFromBucket({\n        prefix: remoteFolder,\n      });\n    } catch (e) {\n      assertError(e);\n      this.logger.error(\n        `Unable to list files for Entity ${entity.metadata.name}: ${e.message}`,\n      );\n    }\n\n    // Then, merge new files into the same folder\n    let absoluteFilesToUpload;\n    try {\n      // Remove the absolute path prefix of the source directory\n      // Path of all files to upload, relative to the root of the source directory\n      // e.g. ['index.html', 'sub-page/index.html', 'assets/images/favicon.png']\n      absoluteFilesToUpload = await getFileTreeRecursively(directory);\n\n      await bulkStorageOperation(\n        async absoluteFilePath => {\n          const relativeFilePath = path.relative(directory, absoluteFilePath);\n          const fileStream = fs.createReadStream(absoluteFilePath);\n\n          const params = {\n            Bucket: this.bucketName,\n            Key: getCloudPathForLocalPath(\n              entity,\n              relativeFilePath,\n              useLegacyPathCasing,\n              bucketRootPath,\n            ),\n            Body: fileStream,\n            ...(sse && { ServerSideEncryption: sse }),\n          } as aws.S3.PutObjectRequest;\n\n          objects.push(params.Key);\n          return this.storageClient.upload(params).promise();\n        },\n        absoluteFilesToUpload,\n        { concurrencyLimit: 10 },\n      );\n\n      this.logger.info(\n        `Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${absoluteFilesToUpload.length}`,\n      );\n    } catch (e) {\n      const errorMessage = `Unable to upload file(s) to AWS S3. ${e}`;\n      this.logger.error(errorMessage);\n      throw new Error(errorMessage);\n    }\n\n    // Last, try to remove the files that were *only* present previously\n    try {\n      const relativeFilesToUpload = absoluteFilesToUpload.map(\n        absoluteFilePath =>\n          getCloudPathForLocalPath(\n            entity,\n            path.relative(directory, absoluteFilePath),\n            useLegacyPathCasing,\n            bucketRootPath,\n          ),\n      );\n      const staleFiles = getStaleFiles(relativeFilesToUpload, existingFiles);\n\n      await bulkStorageOperation(\n        async relativeFilePath => {\n          return await this.storageClient\n            .deleteObject({\n              Bucket: this.bucketName,\n              Key: relativeFilePath,\n            })\n            .promise();\n        },\n        staleFiles,\n        { concurrencyLimit: 10 },\n      );\n\n      this.logger.info(\n        `Successfully deleted stale files for Entity ${entity.metadata.name}. Total number of files: ${staleFiles.length}`,\n      );\n    } catch (error) {\n      const errorMessage = `Unable to delete file(s) from AWS S3. ${error}`;\n      this.logger.error(errorMessage);\n    }\n    return { objects };\n  }\n\n  async fetchTechDocsMetadata(\n    entityName: CompoundEntityRef,\n  ): Promise<TechDocsMetadata> {\n    try {\n      return await new Promise<TechDocsMetadata>(async (resolve, reject) => {\n        const entityTriplet = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;\n        const entityDir = this.legacyPathCasing\n          ? entityTriplet\n          : lowerCaseEntityTriplet(entityTriplet);\n\n        const entityRootDir = path.posix.join(this.bucketRootPath, entityDir);\n\n        const stream = this.storageClient\n          .getObject({\n            Bucket: this.bucketName,\n            Key: `${entityRootDir}/techdocs_metadata.json`,\n          })\n          .createReadStream();\n\n        try {\n          const techdocsMetadataJson = await streamToBuffer(stream);\n          if (!techdocsMetadataJson) {\n            throw new Error(\n              `Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`,\n            );\n          }\n\n          const techdocsMetadata = JSON5.parse(\n            techdocsMetadataJson.toString('utf-8'),\n          );\n\n          resolve(techdocsMetadata);\n        } catch (err) {\n          assertError(err);\n          this.logger.error(err.message);\n          reject(new Error(err.message));\n        }\n      });\n    } catch (e) {\n      throw new ForwardedError('TechDocs metadata fetch failed', e);\n    }\n  }\n\n  /**\n   * Express route middleware to serve static files on a route in techdocs-backend.\n   */\n  docsRouter(): express.Handler {\n    return async (req, res) => {\n      // Decode and trim the leading forward slash\n      const decodedUri = decodeURI(req.path.replace(/^\\//, ''));\n\n      // Root path is removed from the Uri so that legacy casing can be applied\n      // to the entity triplet without manipulating the root path\n      const decodedUriNoRoot = path.relative(this.bucketRootPath, decodedUri);\n\n      // filePath example - /default/component/documented-component/index.html\n      const filePathNoRoot = this.legacyPathCasing\n        ? decodedUriNoRoot\n        : lowerCaseEntityTripletInStoragePath(decodedUriNoRoot);\n\n      // Re-prepend the root path to the relative file path\n      const filePath = path.posix.join(this.bucketRootPath, filePathNoRoot);\n\n      // Files with different extensions (CSS, HTML) need to be served with different headers\n      const fileExtension = path.extname(filePath);\n      const responseHeaders = getHeadersForFileExtension(fileExtension);\n\n      const stream = this.storageClient\n        .getObject({ Bucket: this.bucketName, Key: filePath })\n        .createReadStream();\n      try {\n        // Inject response headers\n        for (const [headerKey, headerValue] of Object.entries(\n          responseHeaders,\n        )) {\n          res.setHeader(headerKey, headerValue);\n        }\n\n        res.send(await streamToBuffer(stream));\n      } catch (err) {\n        assertError(err);\n        this.logger.warn(\n          `TechDocs S3 router failed to serve static files from bucket ${this.bucketName} at key ${filePath}: ${err.message}`,\n        );\n        res.status(404).send('File Not Found');\n      }\n    };\n  }\n\n  /**\n   * A helper function which checks if index.html of an Entity's docs site is available. This\n   * can be used to verify if there are any pre-generated docs available to serve.\n   */\n  async hasDocsBeenGenerated(entity: Entity): Promise<boolean> {\n    try {\n      const entityTriplet = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;\n      const entityDir = this.legacyPathCasing\n        ? entityTriplet\n        : lowerCaseEntityTriplet(entityTriplet);\n\n      const entityRootDir = path.posix.join(this.bucketRootPath, entityDir);\n\n      await this.storageClient\n        .headObject({\n          Bucket: this.bucketName,\n          Key: `${entityRootDir}/index.html`,\n        })\n        .promise();\n      return Promise.resolve(true);\n    } catch (e) {\n      return Promise.resolve(false);\n    }\n  }\n\n  async migrateDocsCase({\n    removeOriginal = false,\n    concurrency = 25,\n  }): Promise<void> {\n    // Iterate through every file in the root of the publisher.\n    const allObjects = await this.getAllObjectsFromBucket();\n    const limiter = createLimiter(concurrency);\n    await Promise.all(\n      allObjects.map(f =>\n        limiter(async file => {\n          let newPath;\n          try {\n            newPath = lowerCaseEntityTripletInStoragePath(file);\n          } catch (e) {\n            assertError(e);\n            this.logger.warn(e.message);\n            return;\n          }\n\n          // If all parts are already lowercase, ignore.\n          if (file === newPath) {\n            return;\n          }\n\n          try {\n            this.logger.verbose(`Migrating ${file}`);\n            await this.storageClient\n              .copyObject({\n                Bucket: this.bucketName,\n                CopySource: [this.bucketName, file].join('/'),\n                Key: newPath,\n              })\n              .promise();\n\n            if (removeOriginal) {\n              await this.storageClient\n                .deleteObject({\n                  Bucket: this.bucketName,\n                  Key: file,\n                })\n                .promise();\n            }\n          } catch (e) {\n            assertError(e);\n            this.logger.warn(`Unable to migrate ${file}: ${e.message}`);\n          }\n        }, f),\n      ),\n    );\n  }\n\n  /**\n   * Returns a list of all object keys from the configured bucket.\n   */\n  protected async getAllObjectsFromBucket(\n    { prefix } = { prefix: '' },\n  ): Promise<string[]> {\n    const objects: string[] = [];\n    let nextContinuation: string | undefined;\n    let allObjects: ListObjectsV2Output;\n    // Iterate through every file in the root of the publisher.\n    do {\n      allObjects = await this.storageClient\n        .listObjectsV2({\n          Bucket: this.bucketName,\n          ContinuationToken: nextContinuation,\n          ...(prefix ? { Prefix: prefix } : {}),\n        })\n        .promise();\n      objects.push(\n        ...(allObjects.Contents || []).map(f => f.Key || '').filter(f => !!f),\n      );\n      nextContinuation = allObjects.NextContinuationToken;\n    } while (nextContinuation);\n\n    return objects;\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { DefaultAzureCredential } from '@azure/identity';\nimport {\n  BlobServiceClient,\n  ContainerClient,\n  StorageSharedKeyCredential,\n} from '@azure/storage-blob';\nimport { Entity, CompoundEntityRef } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { assertError, ForwardedError } from '@backstage/errors';\nimport express from 'express';\nimport JSON5 from 'json5';\nimport limiterFactory from 'p-limit';\nimport { default as path, default as platformPath } from 'path';\nimport { Logger } from 'winston';\nimport {\n  bulkStorageOperation,\n  getCloudPathForLocalPath,\n  getFileTreeRecursively,\n  getHeadersForFileExtension,\n  lowerCaseEntityTriplet,\n  getStaleFiles,\n  lowerCaseEntityTripletInStoragePath,\n} from './helpers';\nimport {\n  PublisherBase,\n  PublishRequest,\n  PublishResponse,\n  ReadinessResponse,\n  TechDocsMetadata,\n} from './types';\n\n// The number of batches that may be ongoing at the same time.\nconst BATCH_CONCURRENCY = 3;\n\nexport class AzureBlobStoragePublish implements PublisherBase {\n  private readonly storageClient: BlobServiceClient;\n  private readonly containerName: string;\n  private readonly legacyPathCasing: boolean;\n  private readonly logger: Logger;\n\n  constructor(options: {\n    storageClient: BlobServiceClient;\n    containerName: string;\n    legacyPathCasing: boolean;\n    logger: Logger;\n  }) {\n    this.storageClient = options.storageClient;\n    this.containerName = options.containerName;\n    this.legacyPathCasing = options.legacyPathCasing;\n    this.logger = options.logger;\n  }\n\n  static fromConfig(config: Config, logger: Logger): PublisherBase {\n    let containerName = '';\n    try {\n      containerName = config.getString(\n        'techdocs.publisher.azureBlobStorage.containerName',\n      );\n    } catch (error) {\n      throw new Error(\n        \"Since techdocs.publisher.type is set to 'azureBlobStorage' in your app config, \" +\n          'techdocs.publisher.azureBlobStorage.containerName is required.',\n      );\n    }\n\n    let accountName = '';\n    try {\n      accountName = config.getString(\n        'techdocs.publisher.azureBlobStorage.credentials.accountName',\n      );\n    } catch (error) {\n      throw new Error(\n        \"Since techdocs.publisher.type is set to 'azureBlobStorage' in your app config, \" +\n          'techdocs.publisher.azureBlobStorage.credentials.accountName is required.',\n      );\n    }\n\n    // Credentials is an optional config. If missing, default Azure Blob Storage environment variables will be used.\n    // https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-app\n    const accountKey = config.getOptionalString(\n      'techdocs.publisher.azureBlobStorage.credentials.accountKey',\n    );\n\n    let credential;\n    if (accountKey) {\n      credential = new StorageSharedKeyCredential(accountName, accountKey);\n    } else {\n      credential = new DefaultAzureCredential();\n    }\n\n    const storageClient = new BlobServiceClient(\n      `https://${accountName}.blob.core.windows.net`,\n      credential,\n    );\n\n    const legacyPathCasing =\n      config.getOptionalBoolean(\n        'techdocs.legacyUseCaseSensitiveTripletPaths',\n      ) || false;\n\n    return new AzureBlobStoragePublish({\n      storageClient: storageClient,\n      containerName: containerName,\n      legacyPathCasing: legacyPathCasing,\n      logger: logger,\n    });\n  }\n\n  async getReadiness(): Promise<ReadinessResponse> {\n    try {\n      const response = await this.storageClient\n        .getContainerClient(this.containerName)\n        .getProperties();\n\n      if (response._response.status === 200) {\n        return {\n          isAvailable: true,\n        };\n      }\n\n      if (response._response.status >= 400) {\n        this.logger.error(\n          `Failed to retrieve metadata from ${response._response.request.url} with status code ${response._response.status}.`,\n        );\n      }\n    } catch (e) {\n      assertError(e);\n      this.logger.error(`from Azure Blob Storage client library: ${e.message}`);\n    }\n\n    this.logger.error(\n      `Could not retrieve metadata about the Azure Blob Storage container ${this.containerName}. ` +\n        'Make sure that the Azure project and container exist and the access key is setup correctly ' +\n        'techdocs.publisher.azureBlobStorage.credentials defined in app config has correct permissions. ' +\n        'Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage',\n    );\n\n    return { isAvailable: false };\n  }\n\n  /**\n   * Upload all the files from the generated `directory` to the Azure Blob Storage container.\n   * Directory structure used in the container is - entityNamespace/entityKind/entityName/index.html\n   */\n  async publish({\n    entity,\n    directory,\n  }: PublishRequest): Promise<PublishResponse> {\n    const objects: string[] = [];\n    const useLegacyPathCasing = this.legacyPathCasing;\n\n    // First, try to retrieve a list of all individual files currently existing\n    const remoteFolder = getCloudPathForLocalPath(\n      entity,\n      undefined,\n      useLegacyPathCasing,\n    );\n    let existingFiles: string[] = [];\n    try {\n      existingFiles = await this.getAllBlobsFromContainer({\n        prefix: remoteFolder,\n        maxPageSize: BATCH_CONCURRENCY,\n      });\n    } catch (e) {\n      assertError(e);\n      this.logger.error(\n        `Unable to list files for Entity ${entity.metadata.name}: ${e.message}`,\n      );\n    }\n\n    // Then, merge new files into the same folder\n    let absoluteFilesToUpload;\n    let container: ContainerClient;\n    try {\n      // Remove the absolute path prefix of the source directory\n      // Path of all files to upload, relative to the root of the source directory\n      // e.g. ['index.html', 'sub-page/index.html', 'assets/images/favicon.png']\n      absoluteFilesToUpload = await getFileTreeRecursively(directory);\n\n      container = this.storageClient.getContainerClient(this.containerName);\n      const failedOperations: Error[] = [];\n      await bulkStorageOperation(\n        async absoluteFilePath => {\n          const relativeFilePath = path.normalize(\n            path.relative(directory, absoluteFilePath),\n          );\n          const remotePath = getCloudPathForLocalPath(\n            entity,\n            relativeFilePath,\n            useLegacyPathCasing,\n          );\n          objects.push(remotePath);\n          const response = await container\n            .getBlockBlobClient(remotePath)\n            .uploadFile(absoluteFilePath);\n\n          if (response._response.status >= 400) {\n            failedOperations.push(\n              new Error(\n                `Upload failed for ${absoluteFilePath} with status code ${response._response.status}`,\n              ),\n            );\n          }\n\n          return response;\n        },\n        absoluteFilesToUpload,\n        { concurrencyLimit: BATCH_CONCURRENCY },\n      );\n\n      if (failedOperations.length > 0) {\n        throw new Error(\n          failedOperations\n            .map(r => r.message)\n            .filter(Boolean)\n            .join(' '),\n        );\n      }\n\n      this.logger.info(\n        `Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${absoluteFilesToUpload.length}`,\n      );\n    } catch (e) {\n      const errorMessage = `Unable to upload file(s) to Azure. ${e}`;\n      this.logger.error(errorMessage);\n      throw new Error(errorMessage);\n    }\n\n    // Last, try to remove the files that were *only* present previously\n    try {\n      const relativeFilesToUpload = absoluteFilesToUpload.map(\n        absoluteFilePath =>\n          getCloudPathForLocalPath(\n            entity,\n            path.relative(directory, absoluteFilePath),\n            useLegacyPathCasing,\n          ),\n      );\n\n      const staleFiles = getStaleFiles(relativeFilesToUpload, existingFiles);\n\n      await bulkStorageOperation(\n        async relativeFilePath => {\n          return await container.deleteBlob(relativeFilePath);\n        },\n        staleFiles,\n        { concurrencyLimit: BATCH_CONCURRENCY },\n      );\n\n      this.logger.info(\n        `Successfully deleted stale files for Entity ${entity.metadata.name}. Total number of files: ${staleFiles.length}`,\n      );\n    } catch (error) {\n      const errorMessage = `Unable to delete file(s) from Azure. ${error}`;\n      this.logger.error(errorMessage);\n    }\n\n    return { objects };\n  }\n\n  private download(containerName: string, blobPath: string): Promise<Buffer> {\n    return new Promise((resolve, reject) => {\n      const fileStreamChunks: Array<any> = [];\n      this.storageClient\n        .getContainerClient(containerName)\n        .getBlockBlobClient(blobPath)\n        .download()\n        .then(res => {\n          const body = res.readableStreamBody;\n          if (!body) {\n            reject(new Error(`Unable to parse the response data`));\n            return;\n          }\n          body\n            .on('error', reject)\n            .on('data', chunk => {\n              fileStreamChunks.push(chunk);\n            })\n            .on('end', () => {\n              resolve(Buffer.concat(fileStreamChunks));\n            });\n        })\n        .catch(reject);\n    });\n  }\n\n  async fetchTechDocsMetadata(\n    entityName: CompoundEntityRef,\n  ): Promise<TechDocsMetadata> {\n    const entityTriplet = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;\n    const entityRootDir = this.legacyPathCasing\n      ? entityTriplet\n      : lowerCaseEntityTriplet(entityTriplet);\n\n    try {\n      const techdocsMetadataJson = await this.download(\n        this.containerName,\n        `${entityRootDir}/techdocs_metadata.json`,\n      );\n      if (!techdocsMetadataJson) {\n        throw new Error(\n          `Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`,\n        );\n      }\n      const techdocsMetadata = JSON5.parse(\n        techdocsMetadataJson.toString('utf-8'),\n      );\n      return techdocsMetadata;\n    } catch (e) {\n      throw new ForwardedError('TechDocs metadata fetch failed', e);\n    }\n  }\n\n  /**\n   * Express route middleware to serve static files on a route in techdocs-backend.\n   */\n  docsRouter(): express.Handler {\n    return (req, res) => {\n      // Decode and trim the leading forward slash\n      const decodedUri = decodeURI(req.path.replace(/^\\//, ''));\n\n      // filePath example - /default/Component/documented-component/index.html\n      const filePath = this.legacyPathCasing\n        ? decodedUri\n        : lowerCaseEntityTripletInStoragePath(decodedUri);\n\n      // Files with different extensions (CSS, HTML) need to be served with different headers\n      const fileExtension = platformPath.extname(filePath);\n      const responseHeaders = getHeadersForFileExtension(fileExtension);\n\n      this.download(this.containerName, filePath)\n        .then(fileContent => {\n          // Inject response headers\n          for (const [headerKey, headerValue] of Object.entries(\n            responseHeaders,\n          )) {\n            res.setHeader(headerKey, headerValue);\n          }\n          res.send(fileContent);\n        })\n        .catch(e => {\n          this.logger.warn(\n            `TechDocs Azure router failed to serve content from container ${this.containerName} at path ${filePath}: ${e.message}`,\n          );\n          res.status(404).send('File Not Found');\n        });\n    };\n  }\n\n  /**\n   * A helper function which checks if index.html of an Entity's docs site is available. This\n   * can be used to verify if there are any pre-generated docs available to serve.\n   */\n  hasDocsBeenGenerated(entity: Entity): Promise<boolean> {\n    const entityTriplet = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;\n    const entityRootDir = this.legacyPathCasing\n      ? entityTriplet\n      : lowerCaseEntityTriplet(entityTriplet);\n\n    return this.storageClient\n      .getContainerClient(this.containerName)\n      .getBlockBlobClient(`${entityRootDir}/index.html`)\n      .exists();\n  }\n\n  protected async renameBlob(\n    originalName: string,\n    newName: string,\n    removeOriginal = false,\n  ): Promise<void> {\n    const container = this.storageClient.getContainerClient(this.containerName);\n    const blob = container.getBlobClient(newName);\n    const { url } = container.getBlobClient(originalName);\n    const response = await blob.beginCopyFromURL(url);\n    await response.pollUntilDone();\n    if (removeOriginal) {\n      await container.deleteBlob(originalName);\n    }\n  }\n\n  protected async renameBlobToLowerCase(\n    originalPath: string,\n    removeOriginal: boolean,\n  ) {\n    let newPath;\n    try {\n      newPath = lowerCaseEntityTripletInStoragePath(originalPath);\n    } catch (e) {\n      assertError(e);\n      this.logger.warn(e.message);\n      return;\n    }\n\n    if (originalPath === newPath) return;\n    try {\n      this.logger.verbose(`Migrating ${originalPath}`);\n      await this.renameBlob(originalPath, newPath, removeOriginal);\n    } catch (e) {\n      assertError(e);\n      this.logger.warn(`Unable to migrate ${originalPath}: ${e.message}`);\n    }\n  }\n\n  async migrateDocsCase({\n    removeOriginal = false,\n    concurrency = 25,\n  }): Promise<void> {\n    const promises = [];\n    const limiter = limiterFactory(concurrency);\n    const container = this.storageClient.getContainerClient(this.containerName);\n\n    for await (const blob of container.listBlobsFlat()) {\n      promises.push(\n        limiter(\n          this.renameBlobToLowerCase.bind(this),\n          blob.name,\n          removeOriginal,\n        ),\n      );\n    }\n\n    await Promise.all(promises);\n  }\n\n  protected async getAllBlobsFromContainer({\n    prefix,\n    maxPageSize,\n  }: {\n    prefix: string;\n    maxPageSize: number;\n  }): Promise<string[]> {\n    const blobs: string[] = [];\n    const container = this.storageClient.getContainerClient(this.containerName);\n\n    let iterator = container.listBlobsFlat({ prefix }).byPage({ maxPageSize });\n    let response = (await iterator.next()).value;\n\n    do {\n      for (const blob of response?.segment?.blobItems ?? []) {\n        blobs.push(blob.name);\n      }\n      iterator = container\n        .listBlobsFlat({ prefix })\n        .byPage({ continuationToken: response.continuationToken, maxPageSize });\n      response = (await iterator.next()).value;\n    } while (response && response.continuationToken);\n\n    return blobs;\n  }\n}\n","/*\n * Copyright 2021 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { assertError } from '@backstage/errors';\nimport { File } from '@google-cloud/storage';\nimport { Writable } from 'stream';\nimport { Logger } from 'winston';\nimport { lowerCaseEntityTripletInStoragePath } from '../helpers';\n\n/**\n * Writable stream to handle object copy/move operations. This implementation\n * ensures we don't read in files from GCS faster than GCS can copy/move them.\n */\nexport class MigrateWriteStream extends Writable {\n  protected logger: Logger;\n  protected removeOriginal: boolean;\n  protected maxConcurrency: number;\n  protected inFlight = 0;\n\n  constructor(logger: Logger, removeOriginal: boolean, concurrency: number) {\n    super({ objectMode: true });\n    this.logger = logger;\n    this.removeOriginal = removeOriginal;\n    this.maxConcurrency = concurrency;\n  }\n\n  _write(file: File, _encoding: BufferEncoding, next: Function) {\n    let shouldCallNext = true;\n    let newFile;\n    try {\n      newFile = lowerCaseEntityTripletInStoragePath(file.name);\n    } catch (e) {\n      assertError(e);\n      this.logger.warn(e.message);\n      next();\n      return;\n    }\n\n    // If all parts are already lowercase, ignore.\n    if (newFile === file.name) {\n      next();\n      return;\n    }\n\n    // Allow up to n-many files to be migrated at a time.\n    this.inFlight++;\n    if (this.inFlight < this.maxConcurrency) {\n      next();\n      shouldCallNext = false;\n    }\n\n    // Otherwise, copy or move the file.\n    const migrate = this.removeOriginal\n      ? file.move.bind(file)\n      : file.copy.bind(file);\n    this.logger.verbose(`Migrating ${file.name}`);\n    migrate(newFile)\n      .catch(e =>\n        this.logger.warn(`Unable to migrate ${file.name}: ${e.message}`),\n      )\n      .finally(() => {\n        this.inFlight--;\n        if (shouldCallNext) {\n          next();\n        }\n      });\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Entity, CompoundEntityRef } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport { assertError } from '@backstage/errors';\nimport { File, FileExistsResponse, Storage } from '@google-cloud/storage';\nimport express from 'express';\nimport JSON5 from 'json5';\nimport path from 'path';\nimport { Readable } from 'stream';\nimport { Logger } from 'winston';\nimport {\n  getFileTreeRecursively,\n  getHeadersForFileExtension,\n  lowerCaseEntityTriplet,\n  lowerCaseEntityTripletInStoragePath,\n  bulkStorageOperation,\n  getCloudPathForLocalPath,\n  getStaleFiles,\n  normalizeExternalStorageRootPath,\n} from './helpers';\nimport { MigrateWriteStream } from './migrations';\nimport {\n  PublisherBase,\n  PublishRequest,\n  PublishResponse,\n  ReadinessResponse,\n  TechDocsMetadata,\n} from './types';\n\nexport class GoogleGCSPublish implements PublisherBase {\n  private readonly storageClient: Storage;\n  private readonly bucketName: string;\n  private readonly legacyPathCasing: boolean;\n  private readonly logger: Logger;\n  private readonly bucketRootPath: string;\n\n  constructor(options: {\n    storageClient: Storage;\n    bucketName: string;\n    legacyPathCasing: boolean;\n    logger: Logger;\n    bucketRootPath: string;\n  }) {\n    this.storageClient = options.storageClient;\n    this.bucketName = options.bucketName;\n    this.legacyPathCasing = options.legacyPathCasing;\n    this.logger = options.logger;\n    this.bucketRootPath = options.bucketRootPath;\n  }\n\n  static fromConfig(config: Config, logger: Logger): PublisherBase {\n    let bucketName = '';\n    try {\n      bucketName = config.getString('techdocs.publisher.googleGcs.bucketName');\n    } catch (error) {\n      throw new Error(\n        \"Since techdocs.publisher.type is set to 'googleGcs' in your app config, \" +\n          'techdocs.publisher.googleGcs.bucketName is required.',\n      );\n    }\n\n    const bucketRootPath = normalizeExternalStorageRootPath(\n      config.getOptionalString('techdocs.publisher.googleGcs.bucketRootPath') ||\n        '',\n    );\n\n    // Credentials is an optional config. If missing, default GCS environment variables will be used.\n    // Read more here https://cloud.google.com/docs/authentication/production\n    const credentials = config.getOptionalString(\n      'techdocs.publisher.googleGcs.credentials',\n    );\n    let credentialsJson: any = {};\n    if (credentials) {\n      try {\n        credentialsJson = JSON.parse(credentials);\n      } catch (err) {\n        throw new Error(\n          'Error in parsing techdocs.publisher.googleGcs.credentials config to JSON.',\n        );\n      }\n    }\n\n    const storageClient = new Storage({\n      ...(credentials && {\n        projectId: credentialsJson.project_id,\n        credentials: credentialsJson,\n      }),\n    });\n\n    const legacyPathCasing =\n      config.getOptionalBoolean(\n        'techdocs.legacyUseCaseSensitiveTripletPaths',\n      ) || false;\n\n    return new GoogleGCSPublish({\n      storageClient,\n      bucketName,\n      legacyPathCasing,\n      logger,\n      bucketRootPath,\n    });\n  }\n\n  /**\n   * Check if the defined bucket exists. Being able to connect means the configuration is good\n   * and the storage client will work.\n   */\n  async getReadiness(): Promise<ReadinessResponse> {\n    try {\n      await this.storageClient.bucket(this.bucketName).getMetadata();\n      this.logger.info(\n        `Successfully connected to the GCS bucket ${this.bucketName}.`,\n      );\n\n      return {\n        isAvailable: true,\n      };\n    } catch (err) {\n      assertError(err);\n      this.logger.error(\n        `Could not retrieve metadata about the GCS bucket ${this.bucketName}. ` +\n          'Make sure the bucket exists. Also make sure that authentication is setup either by explicitly defining ' +\n          'techdocs.publisher.googleGcs.credentials in app config or by using environment variables. ' +\n          'Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage',\n      );\n      this.logger.error(`from GCS client library: ${err.message}`);\n\n      return { isAvailable: false };\n    }\n  }\n\n  /**\n   * Upload all the files from the generated `directory` to the GCS bucket.\n   * Directory structure used in the bucket is - entityNamespace/entityKind/entityName/index.html\n   */\n  async publish({\n    entity,\n    directory,\n  }: PublishRequest): Promise<PublishResponse> {\n    const objects: string[] = [];\n    const useLegacyPathCasing = this.legacyPathCasing;\n    const bucket = this.storageClient.bucket(this.bucketName);\n    const bucketRootPath = this.bucketRootPath;\n\n    // First, try to retrieve a list of all individual files currently existing\n    let existingFiles: string[] = [];\n    try {\n      const remoteFolder = getCloudPathForLocalPath(\n        entity,\n        undefined,\n        useLegacyPathCasing,\n        bucketRootPath,\n      );\n      existingFiles = await this.getFilesForFolder(remoteFolder);\n    } catch (e) {\n      assertError(e);\n      this.logger.error(\n        `Unable to list files for Entity ${entity.metadata.name}: ${e.message}`,\n      );\n    }\n\n    // Then, merge new files into the same folder\n    let absoluteFilesToUpload;\n    try {\n      // Remove the absolute path prefix of the source directory\n      // Path of all files to upload, relative to the root of the source directory\n      // e.g. ['index.html', 'sub-page/index.html', 'assets/images/favicon.png']\n      absoluteFilesToUpload = await getFileTreeRecursively(directory);\n\n      await bulkStorageOperation(\n        async absoluteFilePath => {\n          const relativeFilePath = path.relative(directory, absoluteFilePath);\n          const destination = getCloudPathForLocalPath(\n            entity,\n            relativeFilePath,\n            useLegacyPathCasing,\n            bucketRootPath,\n          );\n          objects.push(destination);\n          return await bucket.upload(absoluteFilePath, { destination });\n        },\n        absoluteFilesToUpload,\n        { concurrencyLimit: 10 },\n      );\n\n      this.logger.info(\n        `Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${absoluteFilesToUpload.length}`,\n      );\n    } catch (e) {\n      const errorMessage = `Unable to upload file(s) to Google Cloud Storage. ${e}`;\n      this.logger.error(errorMessage);\n      throw new Error(errorMessage);\n    }\n\n    // Last, try to remove the files that were *only* present previously\n    try {\n      const relativeFilesToUpload = absoluteFilesToUpload.map(\n        absoluteFilePath =>\n          getCloudPathForLocalPath(\n            entity,\n            path.relative(directory, absoluteFilePath),\n            useLegacyPathCasing,\n            bucketRootPath,\n          ),\n      );\n      const staleFiles = getStaleFiles(relativeFilesToUpload, existingFiles);\n\n      await bulkStorageOperation(\n        async relativeFilePath => {\n          return await bucket.file(relativeFilePath).delete();\n        },\n        staleFiles,\n        { concurrencyLimit: 10 },\n      );\n\n      this.logger.info(\n        `Successfully deleted stale files for Entity ${entity.metadata.name}. Total number of files: ${staleFiles.length}`,\n      );\n    } catch (error) {\n      const errorMessage = `Unable to delete file(s) from Google Cloud Storage. ${error}`;\n      this.logger.error(errorMessage);\n    }\n\n    return { objects };\n  }\n\n  fetchTechDocsMetadata(\n    entityName: CompoundEntityRef,\n  ): Promise<TechDocsMetadata> {\n    return new Promise((resolve, reject) => {\n      const entityTriplet = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;\n      const entityDir = this.legacyPathCasing\n        ? entityTriplet\n        : lowerCaseEntityTriplet(entityTriplet);\n\n      const entityRootDir = path.posix.join(this.bucketRootPath, entityDir);\n\n      const fileStreamChunks: Array<any> = [];\n      this.storageClient\n        .bucket(this.bucketName)\n        .file(`${entityRootDir}/techdocs_metadata.json`)\n        .createReadStream()\n        .on('error', err => {\n          this.logger.error(err.message);\n          reject(err);\n        })\n        .on('data', chunk => {\n          fileStreamChunks.push(chunk);\n        })\n        .on('end', () => {\n          const techdocsMetadataJson =\n            Buffer.concat(fileStreamChunks).toString('utf-8');\n          resolve(JSON5.parse(techdocsMetadataJson));\n        });\n    });\n  }\n\n  /**\n   * Express route middleware to serve static files on a route in techdocs-backend.\n   */\n  docsRouter(): express.Handler {\n    return (req, res) => {\n      // Decode and trim the leading forward slash\n      const decodedUri = decodeURI(req.path.replace(/^\\//, ''));\n\n      // Root path is removed from the Uri so that legacy casing can be applied\n      // to the entity triplet without manipulating the root path\n      const decodedUriNoRoot = path.relative(this.bucketRootPath, decodedUri);\n\n      const filePathNoRoot = this.legacyPathCasing\n        ? decodedUriNoRoot\n        : lowerCaseEntityTripletInStoragePath(decodedUriNoRoot);\n\n      // Re-prepend the root path to the relative file path\n      const filePath = path.posix.join(this.bucketRootPath, filePathNoRoot);\n\n      // Files with different extensions (CSS, HTML) need to be served with different headers\n      const fileExtension = path.extname(filePath);\n      const responseHeaders = getHeadersForFileExtension(fileExtension);\n\n      // Pipe file chunks directly from storage to client.\n      this.storageClient\n        .bucket(this.bucketName)\n        .file(filePath)\n        .createReadStream()\n        .on('pipe', () => {\n          res.writeHead(200, responseHeaders);\n        })\n        .on('error', err => {\n          this.logger.warn(\n            `TechDocs Google GCS router failed to serve content from bucket ${this.bucketName} at path ${filePath}: ${err.message}`,\n          );\n          // Send a 404 with a meaningful message if possible.\n          if (!res.headersSent) {\n            res.status(404).send('File Not Found');\n          } else {\n            res.destroy();\n          }\n        })\n        .pipe(res);\n    };\n  }\n\n  /**\n   * A helper function which checks if index.html of an Entity's docs site is available. This\n   * can be used to verify if there are any pre-generated docs available to serve.\n   */\n  async hasDocsBeenGenerated(entity: Entity): Promise<boolean> {\n    return new Promise(resolve => {\n      const entityTriplet = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;\n      const entityDir = this.legacyPathCasing\n        ? entityTriplet\n        : lowerCaseEntityTriplet(entityTriplet);\n\n      const entityRootDir = path.posix.join(this.bucketRootPath, entityDir);\n\n      this.storageClient\n        .bucket(this.bucketName)\n        .file(`${entityRootDir}/index.html`)\n        .exists()\n        .then((response: FileExistsResponse) => {\n          resolve(response[0]);\n        })\n        .catch(() => {\n          resolve(false);\n        });\n    });\n  }\n\n  migrateDocsCase({ removeOriginal = false, concurrency = 25 }): Promise<void> {\n    return new Promise((resolve, reject) => {\n      // Iterate through every file in the root of the publisher.\n      const allFileMetadata: Readable = this.storageClient\n        .bucket(this.bucketName)\n        .getFilesStream();\n      const migrateFiles = new MigrateWriteStream(\n        this.logger,\n        removeOriginal,\n        concurrency,\n      );\n      migrateFiles.on('finish', resolve).on('error', reject);\n      allFileMetadata.pipe(migrateFiles).on('error', error => {\n        migrateFiles.destroy();\n        reject(error);\n      });\n    });\n  }\n\n  private getFilesForFolder(folder: string): Promise<string[]> {\n    const fileMetadataStream: Readable = this.storageClient\n      .bucket(this.bucketName)\n      .getFilesStream({ prefix: folder });\n\n    return new Promise((resolve, reject) => {\n      const files: string[] = [];\n\n      fileMetadataStream.on('error', error => {\n        // push file to file array\n        reject(error);\n      });\n\n      fileMetadataStream.on('data', (file: File) => {\n        // push file to file array\n        files.push(file.name);\n      });\n\n      fileMetadataStream.on('end', () => {\n        // resolve promise\n        resolve(files);\n      });\n    });\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport {\n  PluginEndpointDiscovery,\n  resolvePackagePath,\n} from '@backstage/backend-common';\nimport { Entity, CompoundEntityRef } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport express from 'express';\nimport fs from 'fs-extra';\nimport os from 'os';\nimport createLimiter from 'p-limit';\nimport path from 'path';\nimport { Logger } from 'winston';\nimport {\n  PublisherBase,\n  PublishRequest,\n  PublishResponse,\n  ReadinessResponse,\n  TechDocsMetadata,\n} from './types';\nimport {\n  getFileTreeRecursively,\n  getHeadersForFileExtension,\n  lowerCaseEntityTripletInStoragePath,\n} from './helpers';\nimport { assertError } from '@backstage/errors';\n\n// TODO: Use a more persistent storage than node_modules or /tmp directory.\n// Make it configurable with techdocs.publisher.local.publishDirectory\nlet staticDocsDir = '';\ntry {\n  staticDocsDir = resolvePackagePath(\n    '@backstage/plugin-techdocs-backend',\n    'static/docs',\n  );\n} catch (err) {\n  // This will most probably never be used.\n  // The try/catch is introduced so that techdocs-cli can import @backstage/plugin-techdocs-node\n  // on CI/CD without installing techdocs backend plugin.\n  staticDocsDir = os.tmpdir();\n}\n\n/**\n * Local publisher which uses the local filesystem to store the generated static files. It uses a directory\n * called \"static\" at the root of techdocs-backend plugin.\n */\nexport class LocalPublish implements PublisherBase {\n  private readonly legacyPathCasing: boolean;\n  private readonly logger: Logger;\n  private readonly discovery: PluginEndpointDiscovery;\n\n  // TODO: Move the logic of setting staticDocsDir based on config over to\n  // fromConfig, and set the value as a class parameter.\n  constructor(options: {\n    logger: Logger;\n    discovery: PluginEndpointDiscovery;\n    legacyPathCasing: boolean;\n  }) {\n    this.logger = options.logger;\n    this.discovery = options.discovery;\n    this.legacyPathCasing = options.legacyPathCasing;\n  }\n\n  static fromConfig(\n    config: Config,\n    logger: Logger,\n    discovery: PluginEndpointDiscovery,\n  ): PublisherBase {\n    const legacyPathCasing =\n      config.getOptionalBoolean(\n        'techdocs.legacyUseCaseSensitiveTripletPaths',\n      ) || false;\n\n    return new LocalPublish({\n      logger,\n      discovery,\n      legacyPathCasing,\n    });\n  }\n\n  async getReadiness(): Promise<ReadinessResponse> {\n    return {\n      isAvailable: true,\n    };\n  }\n\n  async publish({\n    entity,\n    directory,\n  }: PublishRequest): Promise<PublishResponse> {\n    const entityNamespace = entity.metadata.namespace ?? 'default';\n\n    const publishDir = this.staticEntityPathJoin(\n      entityNamespace,\n      entity.kind,\n      entity.metadata.name,\n    );\n\n    if (!fs.existsSync(publishDir)) {\n      this.logger.info(`Could not find ${publishDir}, creating the directory.`);\n      fs.mkdirSync(publishDir, { recursive: true });\n    }\n\n    try {\n      await fs.copy(directory, publishDir);\n      this.logger.info(`Published site stored at ${publishDir}`);\n    } catch (error) {\n      this.logger.debug(\n        `Failed to copy docs from ${directory} to ${publishDir}`,\n      );\n      throw error;\n    }\n\n    // Generate publish response.\n    const techdocsApiUrl = await this.discovery.getBaseUrl('techdocs');\n    const publishedFilePaths = (await getFileTreeRecursively(publishDir)).map(\n      abs => {\n        return abs.split(`${staticDocsDir}/`)[1];\n      },\n    );\n\n    return {\n      remoteUrl: `${techdocsApiUrl}/static/docs/${encodeURIComponent(\n        entity.metadata.name,\n      )}`,\n      objects: publishedFilePaths,\n    };\n  }\n\n  async fetchTechDocsMetadata(\n    entityName: CompoundEntityRef,\n  ): Promise<TechDocsMetadata> {\n    const metadataPath = this.staticEntityPathJoin(\n      entityName.namespace,\n      entityName.kind,\n      entityName.name,\n      'techdocs_metadata.json',\n    );\n\n    try {\n      return await fs.readJson(metadataPath);\n    } catch (err) {\n      assertError(err);\n      this.logger.error(\n        `Unable to read techdocs_metadata.json at ${metadataPath}. Error: ${err}`,\n      );\n      throw new Error(err.message);\n    }\n  }\n\n  docsRouter(): express.Handler {\n    const router = express.Router();\n\n    // Redirect middleware ensuring that requests to case-sensitive entity\n    // triplet paths are always sent to lower-case versions.\n    router.use((req, res, next) => {\n      // If legacy path casing is on, let the request immediately continue.\n      if (this.legacyPathCasing) {\n        return next();\n      }\n\n      // Generate a lower-case entity triplet path.\n      const [_, namespace, kind, name, ...rest] = req.path.split('/');\n\n      // Ignore non-triplet objects.\n      if (!namespace || !kind || !name) {\n        return next();\n      }\n\n      const newPath = [\n        _,\n        namespace.toLowerCase(),\n        kind.toLowerCase(),\n        name.toLowerCase(),\n        ...rest,\n      ].join('/');\n\n      // If there was no change, then let express.static() handle the request.\n      if (newPath === req.path) {\n        return next();\n      }\n\n      // Otherwise, redirect to the new path.\n      return res.redirect(req.baseUrl + newPath, 301);\n    });\n\n    router.use(\n      express.static(staticDocsDir, {\n        // Handle content-type header the same as all other publishers.\n        setHeaders: (res, filePath) => {\n          const fileExtension = path.extname(filePath);\n          const headers = getHeadersForFileExtension(fileExtension);\n          for (const [header, value] of Object.entries(headers)) {\n            res.setHeader(header, value);\n          }\n        },\n      }),\n    );\n\n    return router;\n  }\n\n  async hasDocsBeenGenerated(entity: Entity): Promise<boolean> {\n    const namespace = entity.metadata.namespace ?? 'default';\n\n    const indexHtmlPath = this.staticEntityPathJoin(\n      namespace,\n      entity.kind,\n      entity.metadata.name,\n      'index.html',\n    );\n\n    // Check if the file exists\n    try {\n      await fs.access(indexHtmlPath, fs.constants.F_OK);\n      return true;\n    } catch (err) {\n      return false;\n    }\n  }\n\n  /**\n   * This code will never run in practice. It is merely here to illustrate how\n   * to implement this method for other storage providers.\n   */\n  async migrateDocsCase({\n    removeOriginal = false,\n    concurrency = 25,\n  }): Promise<void> {\n    // Iterate through every file in the root of the publisher.\n    const files = await getFileTreeRecursively(staticDocsDir);\n    const limit = createLimiter(concurrency);\n\n    await Promise.all(\n      files.map(f =>\n        limit(async file => {\n          const relativeFile = file.replace(`${staticDocsDir}${path.sep}`, '');\n          const newFile = lowerCaseEntityTripletInStoragePath(relativeFile);\n\n          // If all parts are already lowercase, ignore.\n          if (relativeFile === newFile) {\n            return;\n          }\n\n          // Otherwise, copy or move the file.\n          await new Promise<void>(resolve => {\n            const migrate = removeOriginal ? fs.move : fs.copyFile;\n            this.logger.verbose(`Migrating ${relativeFile}`);\n            migrate(file, newFile, err => {\n              if (err) {\n                this.logger.warn(\n                  `Unable to migrate ${relativeFile}: ${err.message}`,\n                );\n              }\n              resolve();\n            });\n          });\n        }, f),\n      ),\n    );\n  }\n\n  /**\n   * Utility wrapper around path.join(), used to control legacy case logic.\n   */\n  protected staticEntityPathJoin(...allParts: string[]): string {\n    if (this.legacyPathCasing) {\n      const [namespace, kind, name, ...parts] = allParts;\n      return path.join(staticDocsDir, namespace, kind, name, ...parts);\n    }\n    const [namespace, kind, name, ...parts] = allParts;\n    return path.join(\n      staticDocsDir,\n      namespace.toLowerCase(),\n      kind.toLowerCase(),\n      name.toLowerCase(),\n      ...parts,\n    );\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\nimport { Entity, CompoundEntityRef } from '@backstage/catalog-model';\nimport { Config } from '@backstage/config';\nimport express from 'express';\nimport fs from 'fs-extra';\nimport JSON5 from 'json5';\nimport createLimiter from 'p-limit';\nimport path from 'path';\nimport { SwiftClient } from '@trendyol-js/openstack-swift-sdk';\nimport { NotFound } from '@trendyol-js/openstack-swift-sdk/lib/types';\nimport { Stream, Readable } from 'stream';\nimport { Logger } from 'winston';\nimport {\n  getFileTreeRecursively,\n  getHeadersForFileExtension,\n  lowerCaseEntityTripletInStoragePath,\n} from './helpers';\nimport {\n  PublisherBase,\n  PublishRequest,\n  PublishResponse,\n  ReadinessResponse,\n  TechDocsMetadata,\n} from './types';\nimport { assertError, ForwardedError } from '@backstage/errors';\n\nconst streamToBuffer = (stream: Stream | Readable): Promise<Buffer> => {\n  return new Promise((resolve, reject) => {\n    try {\n      const chunks: any[] = [];\n      stream.on('data', chunk => chunks.push(chunk));\n      stream.on('error', reject);\n      stream.on('end', () => resolve(Buffer.concat(chunks)));\n    } catch (e) {\n      throw new ForwardedError('Unable to parse the response data', e);\n    }\n  });\n};\n\nconst bufferToStream = (buffer: Buffer): Readable => {\n  const stream = new Readable();\n  stream.push(buffer);\n  stream.push(null);\n  return stream;\n};\n\nexport class OpenStackSwiftPublish implements PublisherBase {\n  private readonly storageClient: SwiftClient;\n  private readonly containerName: string;\n  private readonly logger: Logger;\n\n  constructor(options: {\n    storageClient: SwiftClient;\n    containerName: string;\n    logger: Logger;\n  }) {\n    this.storageClient = options.storageClient;\n    this.containerName = options.containerName;\n    this.logger = options.logger;\n  }\n\n  static fromConfig(config: Config, logger: Logger): PublisherBase {\n    let containerName = '';\n    try {\n      containerName = config.getString(\n        'techdocs.publisher.openStackSwift.containerName',\n      );\n    } catch (error) {\n      throw new Error(\n        \"Since techdocs.publisher.type is set to 'openStackSwift' in your app config, \" +\n          'techdocs.publisher.openStackSwift.containerName is required.',\n      );\n    }\n\n    const openStackSwiftConfig = config.getConfig(\n      'techdocs.publisher.openStackSwift',\n    );\n\n    const storageClient = new SwiftClient({\n      authEndpoint: openStackSwiftConfig.getString('authUrl'),\n      swiftEndpoint: openStackSwiftConfig.getString('swiftUrl'),\n      credentialId: openStackSwiftConfig.getString('credentials.id'),\n      secret: openStackSwiftConfig.getString('credentials.secret'),\n    });\n\n    return new OpenStackSwiftPublish({ storageClient, containerName, logger });\n  }\n\n  /*\n   * Check if the defined container exists. Being able to connect means the configuration is good\n   * and the storage client will work.\n   */\n  async getReadiness(): Promise<ReadinessResponse> {\n    try {\n      const container = await this.storageClient.getContainerMetadata(\n        this.containerName,\n      );\n\n      if (!(container instanceof NotFound)) {\n        this.logger.info(\n          `Successfully connected to the OpenStack Swift container ${this.containerName}.`,\n        );\n        return {\n          isAvailable: true,\n        };\n      }\n      this.logger.error(\n        `Could not retrieve metadata about the OpenStack Swift container ${this.containerName}. ` +\n          'Make sure the container exists. Also make sure that authentication is setup either by ' +\n          'explicitly defining credentials and region in techdocs.publisher.openStackSwift in app config or ' +\n          'by using environment variables. Refer to https://backstage.io/docs/features/techdocs/using-cloud-storage',\n      );\n      return {\n        isAvailable: false,\n      };\n    } catch (err) {\n      assertError(err);\n      this.logger.error(`from OpenStack client library: ${err.message}`);\n      return {\n        isAvailable: false,\n      };\n    }\n  }\n\n  /**\n   * Upload all the files from the generated `directory` to the OpenStack Swift container.\n   * Directory structure used in the bucket is - entityNamespace/entityKind/entityName/index.html\n   */\n  async publish({\n    entity,\n    directory,\n  }: PublishRequest): Promise<PublishResponse> {\n    try {\n      const objects: string[] = [];\n\n      // Note: OpenStack Swift manages creation of parent directories if they do not exist.\n      // So collecting path of only the files is good enough.\n      const allFilesToUpload = await getFileTreeRecursively(directory);\n      const limiter = createLimiter(10);\n      const uploadPromises: Array<Promise<unknown>> = [];\n      for (const filePath of allFilesToUpload) {\n        // Remove the absolute path prefix of the source directory\n        // Path of all files to upload, relative to the root of the source directory\n        // e.g. ['index.html', 'sub-page/index.html', 'assets/images/favicon.png']\n        const relativeFilePath = path.relative(directory, filePath);\n        // Convert destination file path to a POSIX path for uploading.\n        // Swift expects / as path separator and relativeFilePath will contain \\\\ on Windows.\n        // https://docs.openstack.org/python-openstackclient/pike/cli/man/openstack.html\n        const relativeFilePathPosix = relativeFilePath\n          .split(path.sep)\n          .join(path.posix.sep);\n\n        // The / delimiter is intentional since it represents the cloud storage and not the local file system.\n        const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;\n        const destination = `${entityRootDir}/${relativeFilePathPosix}`; // Swift container file relative path\n        objects.push(destination);\n\n        // Rate limit the concurrent execution of file uploads to batches of 10 (per publish)\n        const uploadFile = limiter(async () => {\n          const fileBuffer = await fs.readFile(filePath);\n          const stream = bufferToStream(fileBuffer);\n          return this.storageClient.upload(\n            this.containerName,\n            destination,\n            stream,\n          );\n        });\n        uploadPromises.push(uploadFile);\n      }\n      await Promise.all(uploadPromises);\n      this.logger.info(\n        `Successfully uploaded all the generated files for Entity ${entity.metadata.name}. Total number of files: ${allFilesToUpload.length}`,\n      );\n      return { objects };\n    } catch (e) {\n      const errorMessage = `Unable to upload file(s) to OpenStack Swift. ${e}`;\n      this.logger.error(errorMessage);\n      throw new Error(errorMessage);\n    }\n  }\n\n  async fetchTechDocsMetadata(\n    entityName: CompoundEntityRef,\n  ): Promise<TechDocsMetadata> {\n    return await new Promise<TechDocsMetadata>(async (resolve, reject) => {\n      const entityRootDir = `${entityName.namespace}/${entityName.kind}/${entityName.name}`;\n\n      const downloadResponse = await this.storageClient.download(\n        this.containerName,\n        `${entityRootDir}/techdocs_metadata.json`,\n      );\n\n      if (!(downloadResponse instanceof NotFound)) {\n        const stream = downloadResponse.data;\n        try {\n          const techdocsMetadataJson = await streamToBuffer(stream);\n          if (!techdocsMetadataJson) {\n            throw new Error(\n              `Unable to parse the techdocs metadata file ${entityRootDir}/techdocs_metadata.json.`,\n            );\n          }\n\n          const techdocsMetadata = JSON5.parse(\n            techdocsMetadataJson.toString('utf-8'),\n          );\n\n          resolve(techdocsMetadata);\n        } catch (err) {\n          assertError(err);\n          this.logger.error(err.message);\n          reject(new Error(err.message));\n        }\n      } else {\n        reject({\n          message: `TechDocs metadata fetch failed, The file /rootDir/${entityRootDir}/techdocs_metadata.json does not exist !`,\n        });\n      }\n    });\n  }\n\n  /**\n   * Express route middleware to serve static files on a route in techdocs-backend.\n   */\n  docsRouter(): express.Handler {\n    return async (req, res) => {\n      // Decode and trim the leading forward slash\n      // filePath example - /default/Component/documented-component/index.html\n      const filePath = decodeURI(req.path.replace(/^\\//, ''));\n\n      // Files with different extensions (CSS, HTML) need to be served with different headers\n      const fileExtension = path.extname(filePath);\n      const responseHeaders = getHeadersForFileExtension(fileExtension);\n\n      const downloadResponse = await this.storageClient.download(\n        this.containerName,\n        filePath,\n      );\n\n      if (!(downloadResponse instanceof NotFound)) {\n        const stream = downloadResponse.data;\n\n        try {\n          // Inject response headers\n          for (const [headerKey, headerValue] of Object.entries(\n            responseHeaders,\n          )) {\n            res.setHeader(headerKey, headerValue);\n          }\n\n          res.send(await streamToBuffer(stream));\n        } catch (err) {\n          assertError(err);\n          this.logger.warn(\n            `TechDocs OpenStack swift router failed to serve content from container ${this.containerName} at path ${filePath}: ${err.message}`,\n          );\n          res.status(404).send('File Not Found');\n        }\n      } else {\n        this.logger.warn(\n          `TechDocs OpenStack swift router failed to serve content from container ${this.containerName} at path ${filePath}: Not found`,\n        );\n        res.status(404).send('File Not Found');\n      }\n    };\n  }\n\n  /**\n   * A helper function which checks if index.html of an Entity's docs site is available. This\n   * can be used to verify if there are any pre-generated docs available to serve.\n   */\n  async hasDocsBeenGenerated(entity: Entity): Promise<boolean> {\n    const entityRootDir = `${entity.metadata.namespace}/${entity.kind}/${entity.metadata.name}`;\n    try {\n      const fileResponse = await this.storageClient.getMetadata(\n        this.containerName,\n        `${entityRootDir}/index.html`,\n      );\n\n      if (!(fileResponse instanceof NotFound)) {\n        return true;\n      }\n      return false;\n    } catch (err) {\n      assertError(err);\n      this.logger.warn(err.message);\n      return false;\n    }\n  }\n\n  async migrateDocsCase({\n    removeOriginal = false,\n    concurrency = 25,\n  }): Promise<void> {\n    // Iterate through every file in the root of the publisher.\n    const allObjects = await this.getAllObjectsFromContainer();\n    const limiter = createLimiter(concurrency);\n    await Promise.all(\n      allObjects.map(f =>\n        limiter(async file => {\n          let newPath;\n          try {\n            newPath = lowerCaseEntityTripletInStoragePath(file);\n          } catch (e) {\n            assertError(e);\n            this.logger.warn(e.message);\n            return;\n          }\n\n          // If all parts are already lowercase, ignore.\n          if (file === newPath) {\n            return;\n          }\n\n          try {\n            this.logger.verbose(`Migrating ${file} to ${newPath}`);\n            await this.storageClient.copy(\n              this.containerName,\n              file,\n              this.containerName,\n              newPath,\n            );\n            if (removeOriginal) {\n              await this.storageClient.delete(this.containerName, file);\n            }\n          } catch (e) {\n            assertError(e);\n            this.logger.warn(`Unable to migrate ${file}: ${e.message}`);\n          }\n        }, f),\n      ),\n    );\n  }\n\n  /**\n   * Returns a list of all object keys from the configured container.\n   */\n  protected async getAllObjectsFromContainer(\n    { prefix } = { prefix: '' },\n  ): Promise<string[]> {\n    let objects: string[] = [];\n    const OSS_MAX_LIMIT = Math.pow(2, 31) - 1;\n\n    const allObjects = await this.storageClient.list(\n      this.containerName,\n      prefix,\n      OSS_MAX_LIMIT,\n    );\n    objects = allObjects.map((object: any) => object.name);\n\n    return objects;\n  }\n}\n","/*\n * Copyright 2020 The Backstage Authors\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\nimport { Config } from '@backstage/config';\nimport { AwsS3Publish } from './awsS3';\nimport { AzureBlobStoragePublish } from './azureBlobStorage';\nimport { GoogleGCSPublish } from './googleStorage';\nimport { LocalPublish } from './local';\nimport { OpenStackSwiftPublish } from './openStackSwift';\nimport { PublisherFactory, PublisherBase, PublisherType } from './types';\n\n/**\n * Factory class to create a TechDocs publisher based on defined publisher type in app config.\n * Uses `techdocs.publisher.type`.\n * @public\n */\nexport class Publisher {\n  /**\n   * Returns a instance of TechDocs publisher\n   * @param config - A Backstage configuration\n   * @param options - Options for configuring the publisher factory\n   */\n  static async fromConfig(\n    config: Config,\n    { logger, discovery }: PublisherFactory,\n  ): Promise<PublisherBase> {\n    const publisherType = (config.getOptionalString(\n      'techdocs.publisher.type',\n    ) ?? 'local') as PublisherType;\n\n    switch (publisherType) {\n      case 'googleGcs':\n        logger.info('Creating Google Storage Bucket publisher for TechDocs');\n        return GoogleGCSPublish.fromConfig(config, logger);\n      case 'awsS3':\n        logger.info('Creating AWS S3 Bucket publisher for TechDocs');\n        return AwsS3Publish.fromConfig(config, logger);\n      case 'azureBlobStorage':\n        logger.info(\n          'Creating Azure Blob Storage Container publisher for TechDocs',\n        );\n        return AzureBlobStoragePublish.fromConfig(config, logger);\n      case 'openStackSwift':\n        logger.info(\n          'Creating OpenStack Swift Container publisher for TechDocs',\n        );\n        return OpenStackSwiftPublish.fromConfig(config, logger);\n      case 'local':\n        logger.info('Creating Local publisher for TechDocs');\n        return LocalPublish.fromConfig(config, logger, discovery);\n      default:\n        logger.info('Creating Local publisher for TechDocs');\n        return LocalPublish.fromConfig(config, logger, discovery);\n    }\n  }\n}\n"],"names":["mime","recursiveReadDir","path","DEFAULT_NAMESPACE","createLimiter","PassThrough","spawn","gitUrlParse","DEFAULT_SCHEMA","Type","fs","ForwardedError","yaml","isChildPath","resolvePath","ScmIntegrations","InputError","parseLocationRef","getEntitySourceLocation","resolveSafeChildPath","streamToBuffer","aws","Credentials","JSON5","StorageSharedKeyCredential","DefaultAzureCredential","BlobServiceClient","platformPath","limiterFactory","Writable","Storage","resolvePackagePath","os","express","stream","Readable","SwiftClient","NotFound"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAyBA,MAAM,6BAA6B,CAAC,QAAwB;AAC1D,QAAM,qBAAqB;AAI3B,MAAI,IAAI,MAAM,iBAAiB;AAC7B,WAAO;AAAA;AAGT,SAAOA,yBAAK,YAAY,QAAQ;AAAA;MAYrB,6BAA6B,CACxC,kBACwB;AACxB,SAAO;AAAA,IACL,gBAAgB,2BAA2B;AAAA;AAAA;MA4BlC,yBAAyB,OACpC,gBACsB;AAEtB,QAAM,WAAW,MAAMC,qCAAiB,aAAa,MAAM,WAAS;AAClE,UAAM,IAAI,MAAM,sCAAsC,MAAM;AAAA;AAE9D,SAAO;AAAA;MAaI,yBAAyB,CAAC,cAA8B;AACnE,QAAM,CAAC,WAAW,MAAM,SAAS,QAAQ,UAAU,MAAMC,yBAAK,MAAM;AACpE,QAAM,iBAAiB,UAAU;AACjC,QAAM,YAAY,KAAK;AACvB,QAAM,YAAY,KAAK;AACvB,SAAO,CAAC,gBAAgB,WAAW,WAAW,GAAG,MAAM,KAAKA,yBAAK,MAAM;AAAA;MAe5D,sCAAsC,CACjD,iBACW;AACX,MAAI,YAAY;AAChB,MAAI,aAAa,SAASA,yBAAK,MAAM,MAAM;AACzC,gBAAY,aAAa,MAAMA,yBAAK,MAAM,KAAK,KAAKA,yBAAK,MAAM;AAAA;AAIjE,QAAM,QAAQ,UAAU,MAAMA,yBAAK,MAAM;AACzC,MAAI,MAAM,OAAO,IAAI;AACnB,UAAM;AAAA;AAIR,MAAI,MAAM,UAAU,GAAG;AACrB,UAAM,IAAI,MACR,0CAA0C;AAAA;AAI9C,SAAO,uBAAuB,MAAM,KAAKA,yBAAK,MAAM;AAAA;MAWzC,mCAAmC,CAAC,cAA8B;AAE7E,MAAI,iBAAiB;AACrB,MAAI,UAAU,WAAWA,yBAAK,MAAM,MAAM;AACxC,qBAAiB,UAAU,MAAM;AAAA;AAInC,MAAI,eAAe,SAASA,yBAAK,MAAM,MAAM;AAC3C,qBAAiB,eAAe,MAAM,GAAG,eAAe,SAAS;AAAA;AAGnE,SAAO;AAAA;MAII,gBAAgB,CAC3B,UACA,aACa;AACb,QAAM,aAAa,IAAI,IAAI;AAC3B,WAAS,QAAQ,aAAW;AAC1B,eAAW,OAAO;AAAA;AAEpB,SAAO,MAAM,KAAK;AAAA;MAIP,2BAA2B,CACtC,QACA,YAAY,IACZ,sBAAsB,OACtB,0BAA0B,OACf;AAvLb;AA2LE,QAAM,wBAAwB,UAAU,MAAMA,yBAAK,KAAK,KAAKA,yBAAK,MAAM;AAGxE,QAAM,gBAAgB,GAAG,mBAAO,aAAP,mBAAiB,cAAjB,YAA8BC,kCACrD,OAAO,QACL,OAAO,SAAS;AAEpB,QAAM,0BAA0B,GAAG,iBAAiB;AAEpD,QAAM,cAAc,sBAChB,0BACA,uBAAuB;AAG3B,QAAM,sBAAsB;AAAA,IAE1B,GAAG,wBAAwB,MAAMD,yBAAK,MAAM,KAAK,OAAO,OAAK,MAAM;AAAA,IACnE;AAAA,IACA,KAAK;AAEP,SAAO;AAAA;MAII,uBAAuB,OAClC,WACA,MACA,EAAE,qBAAqB,EAAE,kBAAkB,SACxC;AACH,QAAM,UAAUE,kCAAc;AAC9B,QAAM,QAAQ,IAAI,KAAK,IAAI,SAAO,QAAQ,WAAW;AAAA;;yBCzLvB,QAAuC;AACrE,MAAI,CAAC,QAAQ;AACX,UAAM,IAAI,MAAM;AAAA;AAGlB,SAAO;AAAA;MAiBI,aAAa,OAAO;AAAA,EAC/B;AAAA,EACA;AAAA,EACA;AAAA,EACA,YAAY,IAAIC;AAAA,MACO;AACvB,QAAM,IAAI,QAAc,CAAC,SAAS,WAAW;AAC3C,UAAM,UAAUC,oBAAM,SAAS,MAAM;AAErC,YAAQ,OAAO,GAAG,QAAQ,YAAU;AAClC,gBAAU,MAAM;AAAA;AAGlB,YAAQ,OAAO,GAAG,QAAQ,YAAU;AAClC,gBAAU,MAAM;AAAA;AAGlB,YAAQ,GAAG,SAAS,WAAS;AAC3B,aAAO,OAAO;AAAA;AAGhB,YAAQ,GAAG,SAAS,UAAQ;AAC1B,UAAI,SAAS,GAAG;AACd,eAAO,OAAO,WAAW,8BAA8B;AAAA;AAEzD,aAAO;AAAA;AAAA;AAAA;MAcA,mCAAmC,CAC9C,0BACA,iBACA,aAAqB,WACwB;AAC7C,QAAM,EAAE,MAAM,cAAc,WAAW;AAEvC,MAAI,iBAAiB,OAAO;AAC1B,UAAM,cAAc,gBAAgB,MAAM;AAI1C,QAAI,eAAe,CAAC,UAAU,UAAU,SAAS,YAAY,OAAO;AAElE,YAAM,EAAE,iBAAiBC,gCAAY;AACrC,UAAI,iBAAiB,IAAI;AACvB,eAAO,EAAE,UAAU;AAAA;AAGrB,YAAM,eAAe,YAAY,WAAW;AAAA,QAC1C,KAAK,KAAK;AAAA,QACV,MAAM;AAAA;AAER,aAAO,EAAE,UAAU,YAAY,eAAe;AAAA;AAAA;AAIlD,SAAO;AAAA;AAGT,iBAAiB;AAAA,EACf,YAA4B,MAA2B,MAAe;AAA1C;AAA2B;AAAA;AAAA;MAG5C,gBAAgBC,oBAAe,OAAO;AAAA,EACjD,IAAIC,UAAK,IAAI;AAAA,IACX,MAAM;AAAA,IACN,OAAO;AAAA,IACP,eAAe,OAAM,EAAiB;AAAA,IACtC,WAAW,OAAE;AApIjB;AAoIqB,qBAAiB,SAAjB,YAAyB;AAAA;AAAA,IAC1C,YAAY;AAAA,IACZ,WAAW,CAAC,MAAc,SAAkB,IAAI,WAAW,MAAM;AAAA;AAAA;MAWxD,eAAe,OAC1B,aAC+C;AAC/C,MAAI;AACJ,MAAI;AACJ,MAAI;AACF,oBAAgBP,yBAAK,KAAK,UAAU;AACpC,0BAAsB,MAAMQ,uBAAG,SAAS,eAAe;AAAA,UACvD;AACA,QAAI;AACF,sBAAgBR,yBAAK,KAAK,UAAU;AACpC,4BAAsB,MAAMQ,uBAAG,SAAS,eAAe;AAAA,aAChD,OAAP;AACA,YAAM,IAAIC,sBACR,mFACA;AAAA;AAAA;AAKN,SAAO;AAAA,IACL,MAAM;AAAA,IACN,SAAS;AAAA;AAAA;MAaA,qBAAqB,OAChC,UACA,wBACgC;AAChC,QAAM,YAAYC,yBAAK,KAAK,qBAAqB;AAAA,IAC/C,QAAQ;AAAA;AAGV,MAAI,cAAc,QAAQ,OAAO,cAAc,UAAU;AACvD,WAAO;AAAA;AAGT,QAAM,kBAAuC;AAC7C,MACE,gBAAgB,YAChB,CAACC,0BAAY,UAAUC,aAAY,UAAU,gBAAgB,YAC7D;AACA,UAAM,IAAI,MACR;AAAA;AAAA;AAIJ,SAAO,gBAAgB;AAAA;MAQZ,qBAAqB,OAAO;AAAA,EACvC;AAAA,EACA;AAAA,EACA,UAAU;AAAA,MAKN;AACJ,QAAM,WAAWZ,yBAAK,KAAK,UAAU;AACrC,QAAM,cAAcA,yBAAK,KAAK,UAAU;AAExC,MAAI,MAAMQ,uBAAG,WAAW,cAAc;AACpC;AAAA;AAEF,SAAO,KAAK,GAAGR,yBAAK,KAAK,SAAS;AAClC,QAAM,YAAY;AAAA,IAChBA,yBAAK,KAAK,UAAU;AAAA,IACpBA,yBAAK,KAAK,UAAU;AAAA,IACpBA,yBAAK,KAAK,UAAU;AAAA,IACpBA,yBAAK,KAAK,UAAU;AAAA;AAGtB,QAAMQ,uBAAG,UAAU;AACnB,aAAW,YAAY,WAAW;AAChC,QAAI;AACF,YAAMA,uBAAG,SAAS,UAAU;AAC5B;AAAA,aACO,OAAP;AACA,aAAO,KAAK,GAAGR,yBAAK,SAAS,UAAU;AAAA;AAAA;AAI3C,SAAO,KACL,6EAA6E;AAAA,IAC3E;AAAA,IACA,GAAG;AAAA,IACH,KAAK;AAAA;MAWE,yBAAyB,OACpC,sBACA,WACkB;AAClB,QAAM,sBAAsB,qBACzB,MAAMA,yBAAK,KACX,MAAM,GAAG,IACT,KAAKA,yBAAK;AAEb,MAAI;AACF,UAAMQ,uBAAG,OAAO,sBAAsBA,uBAAG,UAAU;AAAA,WAC5C,KAAP;AAEA,UAAMA,uBAAG,UAAU,sBAAsB,KAAK,MAAM;AAAA;AAGtD,MAAI;AACJ,MAAI;AACF,WAAO,MAAMA,uBAAG,SAAS;AAAA,WAClB,KAAP;AACA,uBAAY;AACZ,UAAM,UAAU,mBAAmB,mCAAmC,IAAI;AAC1E,WAAO,MAAM;AACb,UAAM,IAAI,MAAM;AAAA;AAGlB,OAAK,kBAAkB,KAAK;AAI5B,MAAI;AACF,SAAK,QAAS,OAAM,uBAAuB,sBAAsB,IAAI,UACnE,KAAK,QAAQ,GAAG,sBAAsBR,yBAAK,OAAO;AAAA,WAE7C,KAAP;AACA,uBAAY;AACZ,SAAK,QAAQ;AACb,WAAO,KAAK,yCAAyC,IAAI;AAAA;AAG3D,QAAMQ,uBAAG,UAAU,sBAAsB;AACzC;AAAA;MAWW,oBAAoB,OAC/B,sBACA,SACkB;AAClB,QAAM,OAAO,MAAMA,uBAAG,SAAS;AAC/B,OAAK,OAAO;AACZ,QAAMA,uBAAG,UAAU,sBAAsB;AAAA;;AC9R3C,MAAM,kBAAkB,OACtB,eACA,QACA,iBACG;AAGH,MAAI,UAAU;AAEd,MAAI;AACJ,MAAI;AACF,0BAAsB,MAAMA,uBAAG,SAAS,eAAe;AAAA,WAChD,OAAP;AACA,uBAAY;AACZ,WAAO,KACL,0CAA0C,+CAA+C,MAAM;AAEjG;AAAA;AAGF,MAAI;AACJ,MAAI;AACF,gBAAYE,yBAAK,KAAK,qBAAqB,EAAE,QAAQ;AAIrD,QAAI,OAAO,cAAc,YAAY,OAAO,cAAc,aAAa;AACrE,YAAM,IAAI,MAAM;AAAA;AAAA,WAEX,OAAP;AACA,uBAAY;AACZ,WAAO,KACL,4BAA4B,+CAA+C,MAAM;AAEnF;AAAA;AAGF,YAAU,aAAa;AAEvB,MAAI;AACF,QAAI,SAAS;AACX,YAAMF,uBAAG,UACP,eACAE,yBAAK,KAAK,WAAW,EAAE,QAAQ,kBAC/B;AAAA;AAAA,WAGG,OAAP;AACA,uBAAY;AACZ,WAAO,KACL,sBAAsB,iEAAiE,MAAM;AAE/F;AAAA;AAAA;MAqBS,yBAAyB,OACpC,eACA,QACA,0BACA,oBACG;AACH,QAAM,gBAAgB,eAAe,QAAQ,eAAa;AACxD,QAAI,gBAAgB,cAAc,gBAAgB,YAAY;AAI5D,YAAM,SAAS,iCACb,0BACA,iBACA,UAAU;AAGZ,UAAI,OAAO,YAAY,OAAO,UAAU;AACtC,kBAAU,WAAW,OAAO;AAC5B,kBAAU,WAAW,OAAO;AAE5B,eAAO,KACL,OAAO,KAAK,UACV;AAGJ,eAAO;AAAA;AAAA;AAGX,WAAO;AAAA;AAAA;MAgBE,kCAAkC,OAC7C,eACA,WACG;AACH,QAAM,gBAAgB,eAAe,QAAQ,eAAa;AAExD,QAAI,eAAe,YAAY;AAC7B,gBAAU,UAAU,CAAC;AACrB,aAAO;AAAA;AAGT,QAAI,UAAU,WAAW,CAAC,UAAU,QAAQ,SAAS,kBAAkB;AACrE,gBAAU,QAAQ,KAAK;AACvB,aAAO;AAAA;AAET,WAAO;AAAA;AAAA;;ACjHJ,iCAAiD;AAAA,SAgB/C,WAAW,QAAgB,SAA2B;AAC3D,UAAM,EAAE,iBAAiB,WAAW;AACpC,UAAM,kBAAkBG,4BAAgB,WAAW;AACnD,WAAO,IAAI,mBAAkB;AAAA,MAC3B;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA;AAAA;AAAA,EAIJ,YAAY,SAKT;AACD,SAAK,SAAS,QAAQ;AACtB,SAAK,UAAU,oBAAoB,QAAQ,QAAQ,QAAQ;AAC3D,SAAK,kBAAkB,QAAQ;AAC/B,SAAK,kBAAkB,QAAQ;AAAA;AAAA,QAIpB,IAAI,SAA6C;AA1FhE;AA2FI,UAAM;AAAA,MACJ;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA,QAAQ;AAAA,MACR;AAAA,QACE;AAGJ,UAAM,EAAE,MAAM,eAAe,YAAY,MAAM,aAAa;AAG5D,UAAM,UAAU,MAAM,mBAAmB,UAAU;AAEnD,QAAI,0BAA0B;AAC5B,YAAM,uBACJ,eACA,aACA,0BACA,KAAK;AAEP,YAAM,mBAAmB,EAAE,UAAU,QAAQ,aAAa;AAAA;AAG5D,QAAI,CAAC,KAAK,QAAQ,8BAA8B;AAC9C,YAAM,gCAAgC,eAAe;AAAA;AAIvD,UAAM,YAAY;AAAA,OACf,WAAW;AAAA,OACX,YAAY;AAAA;AAGf,QAAI;AACF,cAAQ,KAAK,QAAQ;AAAA,aACd;AACH,gBAAM,WAAW;AAAA,YACf,SAAS;AAAA,YACT,MAAM,CAAC,SAAS,MAAM,WAAW;AAAA,YACjC,SAAS;AAAA,cACP,KAAK;AAAA;AAAA,YAEP;AAAA;AAEF,sBAAY,KACV,oCAAoC,iBAAiB;AAEvD;AAAA,aACG;AACH,gBAAM,KAAK,gBAAgB,aAAa;AAAA,YACtC,WACE,WAAK,QAAQ,gBAAb,YAA4B,mBAAkB;AAAA,YAChD,MAAM,CAAC,SAAS,MAAM;AAAA,YACtB;AAAA,YACA;AAAA,YACA,YAAY;AAAA,YAGZ,SAAS,EAAE,MAAM;AAAA,YACjB,WAAW,KAAK,QAAQ;AAAA;AAE1B,sBAAY,KACV,oCAAoC,iBAAiB;AAEvD;AAAA;AAEA,gBAAM,IAAI,MACR,yBAAyB,KAAK,QAAQ;AAAA;AAAA,aAGrC,OAAP;AACA,WAAK,OAAO,MACV,gCAAgC,iBAAiB;AAEnD,YAAM,IAAIJ,sBACR,gCAAgC,iBAAiB,aACjD;AAAA;AAUJ,UAAM,uBACJT,yBAAK,KAAK,WAAW,2BACrB;AAKF,QAAI,MAAM;AACR,YAAM,kBACJA,yBAAK,KAAK,WAAW,2BACrB;AAAA;AAAA;AAAA;;AA3ID,kBAKkB,qBAAqB;6BA6I5C,QACA,QACiB;AAtMnB;AAuME,QAAM,sBAAsB,OAAO,kBACjC;AAGF,MAAI,qBAAqB;AACvB,WAAO,KACL;AAAA;AAKJ,SAAO;AAAA,IACL,OACE,0DACA,OAAO,kBAAkB,gCADzB,YAEA;AAAA,IACF,aAAa,OAAO,kBAAkB;AAAA,IACtC,WAAW,OAAO,mBAAmB;AAAA,IACrC,8BAA8B,OAAO,mBACnC;AAAA;AAAA;;iBC1L8C;AAAA,EAA7C,cAhCP;AAiCU,4CAAmB;AAAA;AAAA,eAOd,WACX,QACA,SAC2B;AAC3B,UAAM,aAAa,IAAI;AAEvB,UAAM,oBAAoB,kBAAkB,WAAW,QAAQ;AAC/D,eAAW,SAAS,YAAY;AAEhC,WAAO;AAAA;AAAA,EAQT,SAAS,cAAqC,WAA0B;AACtE,SAAK,aAAa,IAAI,cAAc;AAAA;AAAA,EAOtC,IAAI,QAA+B;AACjC,UAAM,eAAe,gBAAgB;AACrC,UAAM,YAAY,KAAK,aAAa,IAAI;AAExC,QAAI,CAAC,WAAW;AACd,YAAM,IAAI,MAAM,wCAAwC;AAAA;AAG1D,WAAO;AAAA;AAAA;;MC9BE,2BAA2B,CACtC,gBACA,WAC6B;AA9C/B;AA+CE,QAAM,aAAa,aAAO,SAAS,gBAAhB,mBAA8B;AACjD,MAAI,CAAC,YAAY;AACf,UAAM,IAAIc,kBACR,8CAA8C,OAAO,SAAS;AAAA;AAIlE,QAAM,EAAE,MAAM,WAAWC,8BAAiB;AAC1C,SAAO;AAAA,IACL;AAAA,IACA;AAAA;AAAA;MAkBS,uBAAuB,CAClC,QACA,eACA,oBAC4C;AAC5C,QAAM,WAAWC,qCAAwB;AAEzC,UAAQ,SAAS;AAAA,SACV,OAAO;AACV,YAAM,SAAS,gBAAgB,WAAW;AAAA,QACxC,KAAK,cAAc;AAAA,QACnB,MAAM,SAAS;AAAA;AAGjB,aAAO;AAAA,QACL,MAAM;AAAA,QACN;AAAA;AAAA;AAAA,SAIC,QAAQ;AAEX,YAAM,SAASC,mCACbjB,yBAAK,QAAQ,SAAS,SACtB,cAAc;AAGhB,aAAO;AAAA,QACL,MAAM;AAAA,QACN;AAAA;AAAA;AAAA;AAKF,YAAM,IAAIc,kBAAW,mCAAmC,SAAS;AAAA;AAAA;MAU1D,uBAAuB,CAClC,QACA,mBAC6B;AAC7B,QAAM,aAAa,yBACjB,6BACA;AAGF,UAAQ,WAAW;AAAA,SACZ;AACH,aAAO;AAAA,SACJ;AACH,aAAO,qBAAqB,QAAQ,YAAY;AAAA;AAEhD,YAAM,IAAI,MAAM,gCAAgC,WAAW;AAAA;AAAA;MAWpD,4BAA4B,OACvC,QACA,QACA,SAC8B;AArJhC;AAsJE,QAAM,EAAE,WAAW,yBACjB,6BACA;AAGF,qCAAM,WAAN,mBAAc,MAAM,sBAAsB;AAE1C,QAAM,mBAAmB,MAAM,OAAO,SAAS,QAAQ,EAAE,MAAM,6BAAM;AACrE,QAAM,cAAc,MAAM,iBAAiB;AAE3C,qCAAM,WAAN,mBAAc,MAAM,iCAAiC;AAErD,SAAO;AAAA,IACL;AAAA,IACA,MAAM,iBAAiB;AAAA;AAAA;;wBC/H4B;AAAA,EAKrD,YAAY,QAAgB,SAAwB,QAAmB;AACrE,SAAK,SAAS;AACd,SAAK,kBAAkBD,4BAAgB,WAAW;AAAA;AAAA,SAQ7C,WACL,QACA,EAAE,QAAQ,UACS;AACnB,WAAO,IAAI,kBAAkB,QAAQ,QAAQ;AAAA;AAAA,QAIzC,QACJ,QACA,SAC2B;AA/D/B;AAgEI,UAAM,aAAa,yBACjB,6BACA;AAEF,UAAM,EAAE,MAAM,WAAW,qBACvB,QACA,YACA,KAAK;AAGP,YAAQ;AAAA,WACD,OAAO;AACV,iDAAS,WAAT,mBAAiB,MAAM,sBAAsB;AAE7C,cAAM,WAAW,MAAM,KAAK,OAAO,SAAS,QAAQ;AAAA,UAClD,MAAM,mCAAS;AAAA;AAEjB,cAAM,cAAc,MAAM,SAAS;AAEnC,iDAAS,WAAT,mBAAiB,MAAM,iCAAiC;AAExD,eAAO;AAAA,UACL;AAAA,UACA,MAAM,SAAS;AAAA;AAAA;AAAA,WAId,OAAO;AACV,eAAO;AAAA,UAEL,aAAa;AAAA,UAEb,MAAM;AAAA;AAAA;AAAA;AAKR,cAAM,IAAIC,kBAAW,mCAAmC;AAAA;AAAA;AAAA;;kBCrEf;AAAA,EAK/C,YAAY,QAAmB,QAAgB;AAC7C,SAAK,SAAS;AACd,SAAK,SAAS;AAAA;AAAA,SAOT,WAAW,EAAE,QAAQ,UAAuC;AACjE,WAAO,IAAI,YAAY,QAAQ;AAAA;AAAA,QAI3B,QACJ,QACA,SAC2B;AAC3B,QAAI;AACF,aAAO,MAAM,0BAA0B,KAAK,QAAQ,QAAQ;AAAA,QAC1D,MAAM,mCAAS;AAAA,QACf,QAAQ,KAAK;AAAA;AAAA,aAER,OAAP;AACA,yBAAY;AAEZ,UAAI,MAAM,SAAS,oBAAoB;AACrC,aAAK,OAAO,MAAM,2BAA2B,mCAAS;AAAA,aACjD;AACL,aAAK,OAAO,MACV,2CAA2C,MAAM;AAAA;AAIrD,YAAM;AAAA;AAAA;AAAA;;gBCxCsC;AAAA,EAA3C,cA/BP;AAgCU,2CAAkB;AAAA;AAAA,eAQb,WACX,iBACA,EAAE,QAAQ,UACgB;AAC1B,UAAM,YAAY,IAAI;AAEtB,UAAM,cAAc,IAAI,YAAY,QAAQ;AAC5C,cAAU,SAAS,OAAO;AAM1B,UAAM,oBAAoB,IAAI,kBAC5B,iBACA,QACA;AAEF,cAAU,SAAS,OAAO;AAE1B,WAAO;AAAA;AAAA,EAQT,SAAS,UAA0B,UAAwB;AACzD,SAAK,YAAY,IAAI,UAAU;AAAA;AAAA,EAQjC,IAAI,QAA8B;AAChC,UAAM,EAAE,SAAS,yBACf,6BACA;AAEF,UAAM,WAAW,KAAK,YAAY,IAAI;AAEtC,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,qCAAqC;AAAA;AAGvD,WAAO;AAAA;AAAA;;AC1CX,MAAMI,mBAAiB,CAAC,WAAsC;AAC5D,SAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,QAAI;AACF,YAAM,SAAgB;AACtB,aAAO,GAAG,QAAQ,WAAS,OAAO,KAAK;AACvC,aAAO,GAAG,SAAS;AACnB,aAAO,GAAG,OAAO,MAAM,QAAQ,OAAO,OAAO;AAAA,aACtC,GAAP;AACA,YAAM,IAAIT,sBAAe,qCAAqC;AAAA;AAAA;AAAA;mBAKjB;AAAA,EAQjD,YAAY,SAOT;AACD,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,aAAa,QAAQ;AAC1B,SAAK,mBAAmB,QAAQ;AAChC,SAAK,SAAS,QAAQ;AACtB,SAAK,iBAAiB,QAAQ;AAC9B,SAAK,MAAM,QAAQ;AAAA;AAAA,SAGd,WAAW,QAAgB,QAA+B;AAC/D,QAAI,aAAa;AACjB,QAAI;AACF,mBAAa,OAAO,UAAU;AAAA,aACvB,OAAP;AACA,YAAM,IAAI,MACR;AAAA;AAKJ,UAAM,iBAAiB,iCACrB,OAAO,kBAAkB,8CAA8C;AAGzE,UAAM,MAAM,OAAO,kBAAkB;AAYrC,UAAM,oBAAoB,OAAO,kBAC/B;AAEF,UAAM,cAAc,aAAa,iBAAiB;AAIlD,UAAM,SAAS,OAAO,kBAAkB;AAIxC,UAAM,WAAW,OAAO,kBACtB;AAKF,UAAM,mBAAmB,OAAO,mBAC9B;AAGF,UAAM,gBAAgB,IAAIU,wBAAI,GAAG;AAAA,MAC/B;AAAA,SACI,UAAU,EAAE;AAAA,SACZ,YAAY,EAAE;AAAA,SACd,oBAAoB,EAAE;AAAA;AAG5B,UAAM,mBACJ,OAAO,mBACL,kDACG;AAEP,WAAO,IAAI,aAAa;AAAA,MACtB;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA;AAAA;AAAA,SAIW,iBACb,QAC8C;AAC9C,QAAI,CAAC,QAAQ;AACX,aAAO;AAAA;AAGT,UAAM,cAAc,OAAO,kBAAkB;AAC7C,UAAM,kBAAkB,OAAO,kBAAkB;AACjD,QAAI;AACJ,QAAI,eAAe,iBAAiB;AAClC,4BAAsB,IAAIC,gBAAY;AAAA,QACpC;AAAA,QACA;AAAA;AAAA;AAIJ,UAAM,UAAU,OAAO,kBAAkB;AACzC,QAAI,SAAS;AACX,aAAO,IAAID,wBAAI,8BAA8B;AAAA,QAC3C,mBAAmB;AAAA,QACnB,QAAQ;AAAA,UACN,iBAAiB;AAAA,UACjB,SAAS;AAAA;AAAA;AAAA;AAKf,WAAO;AAAA;AAAA,QAOH,eAA2C;AAC/C,QAAI;AACF,YAAM,KAAK,cACR,WAAW,EAAE,QAAQ,KAAK,cAC1B;AAEH,WAAK,OAAO,KACV,+CAA+C,KAAK;AAGtD,aAAO,EAAE,aAAa;AAAA,aACf,OAAP;AACA,WAAK,OAAO,MACV,uDAAuD,KAAK;AAK9D,WAAK,OAAO,MAAM,2BAA2B;AAC7C,aAAO;AAAA,QACL,aAAa;AAAA;AAAA;AAAA;AAAA,QASb,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,KAC2C;AAC3C,UAAM,UAAoB;AAC1B,UAAM,sBAAsB,KAAK;AACjC,UAAM,iBAAiB,KAAK;AAC5B,UAAM,MAAM,KAAK;AAGjB,QAAI,gBAA0B;AAC9B,QAAI;AACF,YAAM,eAAe,yBACnB,QACA,QACA,qBACA;AAEF,sBAAgB,MAAM,KAAK,wBAAwB;AAAA,QACjD,QAAQ;AAAA;AAAA,aAEH,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MACV,mCAAmC,OAAO,SAAS,SAAS,EAAE;AAAA;AAKlE,QAAI;AACJ,QAAI;AAIF,8BAAwB,MAAM,uBAAuB;AAErD,YAAM,qBACJ,OAAM,qBAAoB;AACxB,cAAM,mBAAmBnB,yBAAK,SAAS,WAAW;AAClD,cAAM,aAAaQ,uBAAG,iBAAiB;AAEvC,cAAM,SAAS;AAAA,UACb,QAAQ,KAAK;AAAA,UACb,KAAK,yBACH,QACA,kBACA,qBACA;AAAA,UAEF,MAAM;AAAA,aACF,OAAO,EAAE,sBAAsB;AAAA;AAGrC,gBAAQ,KAAK,OAAO;AACpB,eAAO,KAAK,cAAc,OAAO,QAAQ;AAAA,SAE3C,uBACA,EAAE,kBAAkB;AAGtB,WAAK,OAAO,KACV,4DAA4D,OAAO,SAAS,gCAAgC,sBAAsB;AAAA,aAE7H,GAAP;AACA,YAAM,eAAe,uCAAuC;AAC5D,WAAK,OAAO,MAAM;AAClB,YAAM,IAAI,MAAM;AAAA;AAIlB,QAAI;AACF,YAAM,wBAAwB,sBAAsB,IAClD,sBACE,yBACE,QACAR,yBAAK,SAAS,WAAW,mBACzB,qBACA;AAGN,YAAM,aAAa,cAAc,uBAAuB;AAExD,YAAM,qBACJ,OAAM,qBAAoB;AACxB,eAAO,MAAM,KAAK,cACf,aAAa;AAAA,UACZ,QAAQ,KAAK;AAAA,UACb,KAAK;AAAA,WAEN;AAAA,SAEL,YACA,EAAE,kBAAkB;AAGtB,WAAK,OAAO,KACV,+CAA+C,OAAO,SAAS,gCAAgC,WAAW;AAAA,aAErG,OAAP;AACA,YAAM,eAAe,yCAAyC;AAC9D,WAAK,OAAO,MAAM;AAAA;AAEpB,WAAO,EAAE;AAAA;AAAA,QAGL,sBACJ,YAC2B;AAC3B,QAAI;AACF,aAAO,MAAM,IAAI,QAA0B,OAAO,SAAS,WAAW;AACpE,cAAM,gBAAgB,GAAG,WAAW,aAAa,WAAW,QAAQ,WAAW;AAC/E,cAAM,YAAY,KAAK,mBACnB,gBACA,uBAAuB;AAE3B,cAAM,gBAAgBA,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAE3D,cAAM,SAAS,KAAK,cACjB,UAAU;AAAA,UACT,QAAQ,KAAK;AAAA,UACb,KAAK,GAAG;AAAA,WAET;AAEH,YAAI;AACF,gBAAM,uBAAuB,MAAMkB,iBAAe;AAClD,cAAI,CAAC,sBAAsB;AACzB,kBAAM,IAAI,MACR,8CAA8C;AAAA;AAIlD,gBAAM,mBAAmBG,0BAAM,MAC7B,qBAAqB,SAAS;AAGhC,kBAAQ;AAAA,iBACD,KAAP;AACA,6BAAY;AACZ,eAAK,OAAO,MAAM,IAAI;AACtB,iBAAO,IAAI,MAAM,IAAI;AAAA;AAAA;AAAA,aAGlB,GAAP;AACA,YAAM,IAAIZ,sBAAe,kCAAkC;AAAA;AAAA;AAAA,EAO/D,aAA8B;AAC5B,WAAO,OAAO,KAAK,QAAQ;AAEzB,YAAM,aAAa,UAAU,IAAI,KAAK,QAAQ,OAAO;AAIrD,YAAM,mBAAmBT,yBAAK,SAAS,KAAK,gBAAgB;AAG5D,YAAM,iBAAiB,KAAK,mBACxB,mBACA,oCAAoC;AAGxC,YAAM,WAAWA,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAGtD,YAAM,gBAAgBA,yBAAK,QAAQ;AACnC,YAAM,kBAAkB,2BAA2B;AAEnD,YAAM,SAAS,KAAK,cACjB,UAAU,EAAE,QAAQ,KAAK,YAAY,KAAK,YAC1C;AACH,UAAI;AAEF,mBAAW,CAAC,WAAW,gBAAgB,OAAO,QAC5C,kBACC;AACD,cAAI,UAAU,WAAW;AAAA;AAG3B,YAAI,KAAK,MAAMkB,iBAAe;AAAA,eACvB,KAAP;AACA,2BAAY;AACZ,aAAK,OAAO,KACV,+DAA+D,KAAK,qBAAqB,aAAa,IAAI;AAE5G,YAAI,OAAO,KAAK,KAAK;AAAA;AAAA;AAAA;AAAA,QASrB,qBAAqB,QAAkC;AAC3D,QAAI;AACF,YAAM,gBAAgB,GAAG,OAAO,SAAS,aAAa,OAAO,QAAQ,OAAO,SAAS;AACrF,YAAM,YAAY,KAAK,mBACnB,gBACA,uBAAuB;AAE3B,YAAM,gBAAgBlB,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAE3D,YAAM,KAAK,cACR,WAAW;AAAA,QACV,QAAQ,KAAK;AAAA,QACb,KAAK,GAAG;AAAA,SAET;AACH,aAAO,QAAQ,QAAQ;AAAA,aAChB,GAAP;AACA,aAAO,QAAQ,QAAQ;AAAA;AAAA;AAAA,QAIrB,gBAAgB;AAAA,IACpB,iBAAiB;AAAA,IACjB,cAAc;AAAA,KACE;AAEhB,UAAM,aAAa,MAAM,KAAK;AAC9B,UAAM,UAAUE,kCAAc;AAC9B,UAAM,QAAQ,IACZ,WAAW,IAAI,OACb,QAAQ,OAAM,SAAQ;AACpB,UAAI;AACJ,UAAI;AACF,kBAAU,oCAAoC;AAAA,eACvC,GAAP;AACA,2BAAY;AACZ,aAAK,OAAO,KAAK,EAAE;AACnB;AAAA;AAIF,UAAI,SAAS,SAAS;AACpB;AAAA;AAGF,UAAI;AACF,aAAK,OAAO,QAAQ,aAAa;AACjC,cAAM,KAAK,cACR,WAAW;AAAA,UACV,QAAQ,KAAK;AAAA,UACb,YAAY,CAAC,KAAK,YAAY,MAAM,KAAK;AAAA,UACzC,KAAK;AAAA,WAEN;AAEH,YAAI,gBAAgB;AAClB,gBAAM,KAAK,cACR,aAAa;AAAA,YACZ,QAAQ,KAAK;AAAA,YACb,KAAK;AAAA,aAEN;AAAA;AAAA,eAEE,GAAP;AACA,2BAAY;AACZ,aAAK,OAAO,KAAK,qBAAqB,SAAS,EAAE;AAAA;AAAA,OAElD;AAAA;AAAA,QAQO,wBACd,EAAE,WAAW,EAAE,QAAQ,MACJ;AACnB,UAAM,UAAoB;AAC1B,QAAI;AACJ,QAAI;AAEJ,OAAG;AACD,mBAAa,MAAM,KAAK,cACrB,cAAc;AAAA,QACb,QAAQ,KAAK;AAAA,QACb,mBAAmB;AAAA,WACf,SAAS,EAAE,QAAQ,WAAW;AAAA,SAEnC;AACH,cAAQ,KACN,GAAI,YAAW,YAAY,IAAI,IAAI,OAAK,EAAE,OAAO,IAAI,OAAO,OAAK,CAAC,CAAC;AAErE,yBAAmB,WAAW;AAAA,aACvB;AAET,WAAO;AAAA;AAAA;;AChdX,MAAM,oBAAoB;8BAEoC;AAAA,EAM5D,YAAY,SAKT;AACD,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,mBAAmB,QAAQ;AAChC,SAAK,SAAS,QAAQ;AAAA;AAAA,SAGjB,WAAW,QAAgB,QAA+B;AAC/D,QAAI,gBAAgB;AACpB,QAAI;AACF,sBAAgB,OAAO,UACrB;AAAA,aAEK,OAAP;AACA,YAAM,IAAI,MACR;AAAA;AAKJ,QAAI,cAAc;AAClB,QAAI;AACF,oBAAc,OAAO,UACnB;AAAA,aAEK,OAAP;AACA,YAAM,IAAI,MACR;AAAA;AAOJ,UAAM,aAAa,OAAO,kBACxB;AAGF,QAAI;AACJ,QAAI,YAAY;AACd,mBAAa,IAAIoB,uCAA2B,aAAa;AAAA,WACpD;AACL,mBAAa,IAAIC;AAAA;AAGnB,UAAM,gBAAgB,IAAIC,8BACxB,WAAW,qCACX;AAGF,UAAM,mBACJ,OAAO,mBACL,kDACG;AAEP,WAAO,IAAI,wBAAwB;AAAA,MACjC;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA;AAAA;AAAA,QAIE,eAA2C;AAC/C,QAAI;AACF,YAAM,WAAW,MAAM,KAAK,cACzB,mBAAmB,KAAK,eACxB;AAEH,UAAI,SAAS,UAAU,WAAW,KAAK;AACrC,eAAO;AAAA,UACL,aAAa;AAAA;AAAA;AAIjB,UAAI,SAAS,UAAU,UAAU,KAAK;AACpC,aAAK,OAAO,MACV,oCAAoC,SAAS,UAAU,QAAQ,wBAAwB,SAAS,UAAU;AAAA;AAAA,aAGvG,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MAAM,2CAA2C,EAAE;AAAA;AAGjE,SAAK,OAAO,MACV,sEAAsE,KAAK;AAM7E,WAAO,EAAE,aAAa;AAAA;AAAA,QAOlB,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,KAC2C;AAC3C,UAAM,UAAoB;AAC1B,UAAM,sBAAsB,KAAK;AAGjC,UAAM,eAAe,yBACnB,QACA,QACA;AAEF,QAAI,gBAA0B;AAC9B,QAAI;AACF,sBAAgB,MAAM,KAAK,yBAAyB;AAAA,QAClD,QAAQ;AAAA,QACR,aAAa;AAAA;AAAA,aAER,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MACV,mCAAmC,OAAO,SAAS,SAAS,EAAE;AAAA;AAKlE,QAAI;AACJ,QAAI;AACJ,QAAI;AAIF,8BAAwB,MAAM,uBAAuB;AAErD,kBAAY,KAAK,cAAc,mBAAmB,KAAK;AACvD,YAAM,mBAA4B;AAClC,YAAM,qBACJ,OAAM,qBAAoB;AACxB,cAAM,mBAAmBxB,yBAAK,UAC5BA,yBAAK,SAAS,WAAW;AAE3B,cAAM,aAAa,yBACjB,QACA,kBACA;AAEF,gBAAQ,KAAK;AACb,cAAM,WAAW,MAAM,UACpB,mBAAmB,YACnB,WAAW;AAEd,YAAI,SAAS,UAAU,UAAU,KAAK;AACpC,2BAAiB,KACf,IAAI,MACF,qBAAqB,qCAAqC,SAAS,UAAU;AAAA;AAKnF,eAAO;AAAA,SAET,uBACA,EAAE,kBAAkB;AAGtB,UAAI,iBAAiB,SAAS,GAAG;AAC/B,cAAM,IAAI,MACR,iBACG,IAAI,OAAK,EAAE,SACX,OAAO,SACP,KAAK;AAAA;AAIZ,WAAK,OAAO,KACV,4DAA4D,OAAO,SAAS,gCAAgC,sBAAsB;AAAA,aAE7H,GAAP;AACA,YAAM,eAAe,sCAAsC;AAC3D,WAAK,OAAO,MAAM;AAClB,YAAM,IAAI,MAAM;AAAA;AAIlB,QAAI;AACF,YAAM,wBAAwB,sBAAsB,IAClD,sBACE,yBACE,QACAA,yBAAK,SAAS,WAAW,mBACzB;AAIN,YAAM,aAAa,cAAc,uBAAuB;AAExD,YAAM,qBACJ,OAAM,qBAAoB;AACxB,eAAO,MAAM,UAAU,WAAW;AAAA,SAEpC,YACA,EAAE,kBAAkB;AAGtB,WAAK,OAAO,KACV,+CAA+C,OAAO,SAAS,gCAAgC,WAAW;AAAA,aAErG,OAAP;AACA,YAAM,eAAe,wCAAwC;AAC7D,WAAK,OAAO,MAAM;AAAA;AAGpB,WAAO,EAAE;AAAA;AAAA,EAGH,SAAS,eAAuB,UAAmC;AACzE,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,YAAM,mBAA+B;AACrC,WAAK,cACF,mBAAmB,eACnB,mBAAmB,UACnB,WACA,KAAK,SAAO;AACX,cAAM,OAAO,IAAI;AACjB,YAAI,CAAC,MAAM;AACT,iBAAO,IAAI,MAAM;AACjB;AAAA;AAEF,aACG,GAAG,SAAS,QACZ,GAAG,QAAQ,WAAS;AACnB,2BAAiB,KAAK;AAAA,WAEvB,GAAG,OAAO,MAAM;AACf,kBAAQ,OAAO,OAAO;AAAA;AAAA,SAG3B,MAAM;AAAA;AAAA;AAAA,QAIP,sBACJ,YAC2B;AAC3B,UAAM,gBAAgB,GAAG,WAAW,aAAa,WAAW,QAAQ,WAAW;AAC/E,UAAM,gBAAgB,KAAK,mBACvB,gBACA,uBAAuB;AAE3B,QAAI;AACF,YAAM,uBAAuB,MAAM,KAAK,SACtC,KAAK,eACL,GAAG;AAEL,UAAI,CAAC,sBAAsB;AACzB,cAAM,IAAI,MACR,8CAA8C;AAAA;AAGlD,YAAM,mBAAmBqB,0BAAM,MAC7B,qBAAqB,SAAS;AAEhC,aAAO;AAAA,aACA,GAAP;AACA,YAAM,IAAIZ,sBAAe,kCAAkC;AAAA;AAAA;AAAA,EAO/D,aAA8B;AAC5B,WAAO,CAAC,KAAK,QAAQ;AAEnB,YAAM,aAAa,UAAU,IAAI,KAAK,QAAQ,OAAO;AAGrD,YAAM,WAAW,KAAK,mBAClB,aACA,oCAAoC;AAGxC,YAAM,gBAAgBgB,yBAAa,QAAQ;AAC3C,YAAM,kBAAkB,2BAA2B;AAEnD,WAAK,SAAS,KAAK,eAAe,UAC/B,KAAK,iBAAe;AAEnB,mBAAW,CAAC,WAAW,gBAAgB,OAAO,QAC5C,kBACC;AACD,cAAI,UAAU,WAAW;AAAA;AAE3B,YAAI,KAAK;AAAA,SAEV,MAAM,OAAK;AACV,aAAK,OAAO,KACV,gEAAgE,KAAK,yBAAyB,aAAa,EAAE;AAE/G,YAAI,OAAO,KAAK,KAAK;AAAA;AAAA;AAAA;AAAA,EAS7B,qBAAqB,QAAkC;AACrD,UAAM,gBAAgB,GAAG,OAAO,SAAS,aAAa,OAAO,QAAQ,OAAO,SAAS;AACrF,UAAM,gBAAgB,KAAK,mBACvB,gBACA,uBAAuB;AAE3B,WAAO,KAAK,cACT,mBAAmB,KAAK,eACxB,mBAAmB,GAAG,4BACtB;AAAA;AAAA,QAGW,WACd,cACA,SACA,iBAAiB,OACF;AACf,UAAM,YAAY,KAAK,cAAc,mBAAmB,KAAK;AAC7D,UAAM,OAAO,UAAU,cAAc;AACrC,UAAM,EAAE,QAAQ,UAAU,cAAc;AACxC,UAAM,WAAW,MAAM,KAAK,iBAAiB;AAC7C,UAAM,SAAS;AACf,QAAI,gBAAgB;AAClB,YAAM,UAAU,WAAW;AAAA;AAAA;AAAA,QAIf,sBACd,cACA,gBACA;AACA,QAAI;AACJ,QAAI;AACF,gBAAU,oCAAoC;AAAA,aACvC,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,KAAK,EAAE;AACnB;AAAA;AAGF,QAAI,iBAAiB;AAAS;AAC9B,QAAI;AACF,WAAK,OAAO,QAAQ,aAAa;AACjC,YAAM,KAAK,WAAW,cAAc,SAAS;AAAA,aACtC,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,KAAK,qBAAqB,iBAAiB,EAAE;AAAA;AAAA;AAAA,QAIvD,gBAAgB;AAAA,IACpB,iBAAiB;AAAA,IACjB,cAAc;AAAA,KACE;AAChB,UAAM,WAAW;AACjB,UAAM,UAAUC,kCAAe;AAC/B,UAAM,YAAY,KAAK,cAAc,mBAAmB,KAAK;AAE7D,qBAAiB,QAAQ,UAAU,iBAAiB;AAClD,eAAS,KACP,QACE,KAAK,sBAAsB,KAAK,OAChC,KAAK,MACL;AAAA;AAKN,UAAM,QAAQ,IAAI;AAAA;AAAA,QAGJ,yBAAyB;AAAA,IACvC;AAAA,IACA;AAAA,KAIoB;AA7bxB;AA8bI,UAAM,QAAkB;AACxB,UAAM,YAAY,KAAK,cAAc,mBAAmB,KAAK;AAE7D,QAAI,WAAW,UAAU,cAAc,EAAE,UAAU,OAAO,EAAE;AAC5D,QAAI,WAAY,OAAM,SAAS,QAAQ;AAEvC,OAAG;AACD,iBAAW,QAAQ,iDAAU,YAAV,mBAAmB,cAAnB,YAAgC,IAAI;AACrD,cAAM,KAAK,KAAK;AAAA;AAElB,iBAAW,UACR,cAAc,EAAE,UAChB,OAAO,EAAE,mBAAmB,SAAS,mBAAmB;AAC3D,iBAAY,OAAM,SAAS,QAAQ;AAAA,aAC5B,YAAY,SAAS;AAE9B,WAAO;AAAA;AAAA;;iCCpb6BC,gBAAS;AAAA,EAM/C,YAAY,QAAgB,gBAAyB,aAAqB;AACxE,UAAM,EAAE,YAAY;AAHZ,oBAAW;AAInB,SAAK,SAAS;AACd,SAAK,iBAAiB;AACtB,SAAK,iBAAiB;AAAA;AAAA,EAGxB,OAAO,MAAY,WAA2B,MAAgB;AAC5D,QAAI,iBAAiB;AACrB,QAAI;AACJ,QAAI;AACF,gBAAU,oCAAoC,KAAK;AAAA,aAC5C,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,KAAK,EAAE;AACnB;AACA;AAAA;AAIF,QAAI,YAAY,KAAK,MAAM;AACzB;AACA;AAAA;AAIF,SAAK;AACL,QAAI,KAAK,WAAW,KAAK,gBAAgB;AACvC;AACA,uBAAiB;AAAA;AAInB,UAAM,UAAU,KAAK,iBACjB,KAAK,KAAK,KAAK,QACf,KAAK,KAAK,KAAK;AACnB,SAAK,OAAO,QAAQ,aAAa,KAAK;AACtC,YAAQ,SACL,MAAM,OACL,KAAK,OAAO,KAAK,qBAAqB,KAAK,SAAS,EAAE,YAEvD,QAAQ,MAAM;AACb,WAAK;AACL,UAAI,gBAAgB;AAClB;AAAA;AAAA;AAAA;AAAA;;uBCjC6C;AAAA,EAOrD,YAAY,SAMT;AACD,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,aAAa,QAAQ;AAC1B,SAAK,mBAAmB,QAAQ;AAChC,SAAK,SAAS,QAAQ;AACtB,SAAK,iBAAiB,QAAQ;AAAA;AAAA,SAGzB,WAAW,QAAgB,QAA+B;AAC/D,QAAI,aAAa;AACjB,QAAI;AACF,mBAAa,OAAO,UAAU;AAAA,aACvB,OAAP;AACA,YAAM,IAAI,MACR;AAAA;AAKJ,UAAM,iBAAiB,iCACrB,OAAO,kBAAkB,kDACvB;AAKJ,UAAM,cAAc,OAAO,kBACzB;AAEF,QAAI,kBAAuB;AAC3B,QAAI,aAAa;AACf,UAAI;AACF,0BAAkB,KAAK,MAAM;AAAA,eACtB,KAAP;AACA,cAAM,IAAI,MACR;AAAA;AAAA;AAKN,UAAM,gBAAgB,IAAIC,gBAAQ;AAAA,SAC5B,eAAe;AAAA,QACjB,WAAW,gBAAgB;AAAA,QAC3B,aAAa;AAAA;AAAA;AAIjB,UAAM,mBACJ,OAAO,mBACL,kDACG;AAEP,WAAO,IAAI,iBAAiB;AAAA,MAC1B;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA,MACA;AAAA;AAAA;AAAA,QAQE,eAA2C;AAC/C,QAAI;AACF,YAAM,KAAK,cAAc,OAAO,KAAK,YAAY;AACjD,WAAK,OAAO,KACV,4CAA4C,KAAK;AAGnD,aAAO;AAAA,QACL,aAAa;AAAA;AAAA,aAER,KAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MACV,oDAAoD,KAAK;AAK3D,WAAK,OAAO,MAAM,4BAA4B,IAAI;AAElD,aAAO,EAAE,aAAa;AAAA;AAAA;AAAA,QAQpB,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,KAC2C;AAC3C,UAAM,UAAoB;AAC1B,UAAM,sBAAsB,KAAK;AACjC,UAAM,SAAS,KAAK,cAAc,OAAO,KAAK;AAC9C,UAAM,iBAAiB,KAAK;AAG5B,QAAI,gBAA0B;AAC9B,QAAI;AACF,YAAM,eAAe,yBACnB,QACA,QACA,qBACA;AAEF,sBAAgB,MAAM,KAAK,kBAAkB;AAAA,aACtC,GAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MACV,mCAAmC,OAAO,SAAS,SAAS,EAAE;AAAA;AAKlE,QAAI;AACJ,QAAI;AAIF,8BAAwB,MAAM,uBAAuB;AAErD,YAAM,qBACJ,OAAM,qBAAoB;AACxB,cAAM,mBAAmB5B,yBAAK,SAAS,WAAW;AAClD,cAAM,cAAc,yBAClB,QACA,kBACA,qBACA;AAEF,gBAAQ,KAAK;AACb,eAAO,MAAM,OAAO,OAAO,kBAAkB,EAAE;AAAA,SAEjD,uBACA,EAAE,kBAAkB;AAGtB,WAAK,OAAO,KACV,4DAA4D,OAAO,SAAS,gCAAgC,sBAAsB;AAAA,aAE7H,GAAP;AACA,YAAM,eAAe,qDAAqD;AAC1E,WAAK,OAAO,MAAM;AAClB,YAAM,IAAI,MAAM;AAAA;AAIlB,QAAI;AACF,YAAM,wBAAwB,sBAAsB,IAClD,sBACE,yBACE,QACAA,yBAAK,SAAS,WAAW,mBACzB,qBACA;AAGN,YAAM,aAAa,cAAc,uBAAuB;AAExD,YAAM,qBACJ,OAAM,qBAAoB;AACxB,eAAO,MAAM,OAAO,KAAK,kBAAkB;AAAA,SAE7C,YACA,EAAE,kBAAkB;AAGtB,WAAK,OAAO,KACV,+CAA+C,OAAO,SAAS,gCAAgC,WAAW;AAAA,aAErG,OAAP;AACA,YAAM,eAAe,uDAAuD;AAC5E,WAAK,OAAO,MAAM;AAAA;AAGpB,WAAO,EAAE;AAAA;AAAA,EAGX,sBACE,YAC2B;AAC3B,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,YAAM,gBAAgB,GAAG,WAAW,aAAa,WAAW,QAAQ,WAAW;AAC/E,YAAM,YAAY,KAAK,mBACnB,gBACA,uBAAuB;AAE3B,YAAM,gBAAgBA,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAE3D,YAAM,mBAA+B;AACrC,WAAK,cACF,OAAO,KAAK,YACZ,KAAK,GAAG,wCACR,mBACA,GAAG,SAAS,SAAO;AAClB,aAAK,OAAO,MAAM,IAAI;AACtB,eAAO;AAAA,SAER,GAAG,QAAQ,WAAS;AACnB,yBAAiB,KAAK;AAAA,SAEvB,GAAG,OAAO,MAAM;AACf,cAAM,uBACJ,OAAO,OAAO,kBAAkB,SAAS;AAC3C,gBAAQqB,0BAAM,MAAM;AAAA;AAAA;AAAA;AAAA,EAQ5B,aAA8B;AAC5B,WAAO,CAAC,KAAK,QAAQ;AAEnB,YAAM,aAAa,UAAU,IAAI,KAAK,QAAQ,OAAO;AAIrD,YAAM,mBAAmBrB,yBAAK,SAAS,KAAK,gBAAgB;AAE5D,YAAM,iBAAiB,KAAK,mBACxB,mBACA,oCAAoC;AAGxC,YAAM,WAAWA,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAGtD,YAAM,gBAAgBA,yBAAK,QAAQ;AACnC,YAAM,kBAAkB,2BAA2B;AAGnD,WAAK,cACF,OAAO,KAAK,YACZ,KAAK,UACL,mBACA,GAAG,QAAQ,MAAM;AAChB,YAAI,UAAU,KAAK;AAAA,SAEpB,GAAG,SAAS,SAAO;AAClB,aAAK,OAAO,KACV,kEAAkE,KAAK,sBAAsB,aAAa,IAAI;AAGhH,YAAI,CAAC,IAAI,aAAa;AACpB,cAAI,OAAO,KAAK,KAAK;AAAA,eAChB;AACL,cAAI;AAAA;AAAA,SAGP,KAAK;AAAA;AAAA;AAAA,QAQN,qBAAqB,QAAkC;AAC3D,WAAO,IAAI,QAAQ,aAAW;AAC5B,YAAM,gBAAgB,GAAG,OAAO,SAAS,aAAa,OAAO,QAAQ,OAAO,SAAS;AACrF,YAAM,YAAY,KAAK,mBACnB,gBACA,uBAAuB;AAE3B,YAAM,gBAAgBA,yBAAK,MAAM,KAAK,KAAK,gBAAgB;AAE3D,WAAK,cACF,OAAO,KAAK,YACZ,KAAK,GAAG,4BACR,SACA,KAAK,CAAC,aAAiC;AACtC,gBAAQ,SAAS;AAAA,SAElB,MAAM,MAAM;AACX,gBAAQ;AAAA;AAAA;AAAA;AAAA,EAKhB,gBAAgB,EAAE,iBAAiB,OAAO,cAAc,MAAqB;AAC3E,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AAEtC,YAAM,kBAA4B,KAAK,cACpC,OAAO,KAAK,YACZ;AACH,YAAM,eAAe,IAAI,mBACvB,KAAK,QACL,gBACA;AAEF,mBAAa,GAAG,UAAU,SAAS,GAAG,SAAS;AAC/C,sBAAgB,KAAK,cAAc,GAAG,SAAS,WAAS;AACtD,qBAAa;AACb,eAAO;AAAA;AAAA;AAAA;AAAA,EAKL,kBAAkB,QAAmC;AAC3D,UAAM,qBAA+B,KAAK,cACvC,OAAO,KAAK,YACZ,eAAe,EAAE,QAAQ;AAE5B,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,YAAM,QAAkB;AAExB,yBAAmB,GAAG,SAAS,WAAS;AAEtC,eAAO;AAAA;AAGT,yBAAmB,GAAG,QAAQ,CAAC,SAAe;AAE5C,cAAM,KAAK,KAAK;AAAA;AAGlB,yBAAmB,GAAG,OAAO,MAAM;AAEjC,gBAAQ;AAAA;AAAA;AAAA;AAAA;;ACnVhB,IAAI,gBAAgB;AACpB,IAAI;AACF,kBAAgB6B,iCACd,sCACA;AAAA,SAEK,KAAP;AAIA,kBAAgBC,uBAAG;AAAA;mBAO8B;AAAA,EAOjD,YAAY,SAIT;AACD,SAAK,SAAS,QAAQ;AACtB,SAAK,YAAY,QAAQ;AACzB,SAAK,mBAAmB,QAAQ;AAAA;AAAA,SAG3B,WACL,QACA,QACA,WACe;AACf,UAAM,mBACJ,OAAO,mBACL,kDACG;AAEP,WAAO,IAAI,aAAa;AAAA,MACtB;AAAA,MACA;AAAA,MACA;AAAA;AAAA;AAAA,QAIE,eAA2C;AAC/C,WAAO;AAAA,MACL,aAAa;AAAA;AAAA;AAAA,QAIX,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,KAC2C;AAvG/C;AAwGI,UAAM,kBAAkB,aAAO,SAAS,cAAhB,YAA6B;AAErD,UAAM,aAAa,KAAK,qBACtB,iBACA,OAAO,MACP,OAAO,SAAS;AAGlB,QAAI,CAACtB,uBAAG,WAAW,aAAa;AAC9B,WAAK,OAAO,KAAK,kBAAkB;AACnC,6BAAG,UAAU,YAAY,EAAE,WAAW;AAAA;AAGxC,QAAI;AACF,YAAMA,uBAAG,KAAK,WAAW;AACzB,WAAK,OAAO,KAAK,4BAA4B;AAAA,aACtC,OAAP;AACA,WAAK,OAAO,MACV,4BAA4B,gBAAgB;AAE9C,YAAM;AAAA;AAIR,UAAM,iBAAiB,MAAM,KAAK,UAAU,WAAW;AACvD,UAAM,qBAAsB,OAAM,uBAAuB,aAAa,IACpE,SAAO;AACL,aAAO,IAAI,MAAM,GAAG,kBAAkB;AAAA;AAI1C,WAAO;AAAA,MACL,WAAW,GAAG,8BAA8B,mBAC1C,OAAO,SAAS;AAAA,MAElB,SAAS;AAAA;AAAA;AAAA,QAIP,sBACJ,YAC2B;AAC3B,UAAM,eAAe,KAAK,qBACxB,WAAW,WACX,WAAW,MACX,WAAW,MACX;AAGF,QAAI;AACF,aAAO,MAAMA,uBAAG,SAAS;AAAA,aAClB,KAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MACV,4CAA4C,wBAAwB;AAEtE,YAAM,IAAI,MAAM,IAAI;AAAA;AAAA;AAAA,EAIxB,aAA8B;AAC5B,UAAM,SAASuB,4BAAQ;AAIvB,WAAO,IAAI,CAAC,KAAK,KAAK,SAAS;AAE7B,UAAI,KAAK,kBAAkB;AACzB,eAAO;AAAA;AAIT,YAAM,CAAC,GAAG,WAAW,MAAM,SAAS,QAAQ,IAAI,KAAK,MAAM;AAG3D,UAAI,CAAC,aAAa,CAAC,QAAQ,CAAC,MAAM;AAChC,eAAO;AAAA;AAGT,YAAM,UAAU;AAAA,QACd;AAAA,QACA,UAAU;AAAA,QACV,KAAK;AAAA,QACL,KAAK;AAAA,QACL,GAAG;AAAA,QACH,KAAK;AAGP,UAAI,YAAY,IAAI,MAAM;AACxB,eAAO;AAAA;AAIT,aAAO,IAAI,SAAS,IAAI,UAAU,SAAS;AAAA;AAG7C,WAAO,IACLA,4BAAQ,OAAO,eAAe;AAAA,MAE5B,YAAY,CAAC,KAAK,aAAa;AAC7B,cAAM,gBAAgB/B,yBAAK,QAAQ;AACnC,cAAM,UAAU,2BAA2B;AAC3C,mBAAW,CAAC,QAAQ,UAAU,OAAO,QAAQ,UAAU;AACrD,cAAI,UAAU,QAAQ;AAAA;AAAA;AAAA;AAM9B,WAAO;AAAA;AAAA,QAGH,qBAAqB,QAAkC;AAxN/D;AAyNI,UAAM,YAAY,aAAO,SAAS,cAAhB,YAA6B;AAE/C,UAAM,gBAAgB,KAAK,qBACzB,WACA,OAAO,MACP,OAAO,SAAS,MAChB;AAIF,QAAI;AACF,YAAMQ,uBAAG,OAAO,eAAeA,uBAAG,UAAU;AAC5C,aAAO;AAAA,aACA,KAAP;AACA,aAAO;AAAA;AAAA;AAAA,QAQL,gBAAgB;AAAA,IACpB,iBAAiB;AAAA,IACjB,cAAc;AAAA,KACE;AAEhB,UAAM,QAAQ,MAAM,uBAAuB;AAC3C,UAAM,QAAQN,kCAAc;AAE5B,UAAM,QAAQ,IACZ,MAAM,IAAI,OACR,MAAM,OAAM,SAAQ;AAClB,YAAM,eAAe,KAAK,QAAQ,GAAG,gBAAgBF,yBAAK,OAAO;AACjE,YAAM,UAAU,oCAAoC;AAGpD,UAAI,iBAAiB,SAAS;AAC5B;AAAA;AAIF,YAAM,IAAI,QAAc,aAAW;AACjC,cAAM,UAAU,iBAAiBQ,uBAAG,OAAOA,uBAAG;AAC9C,aAAK,OAAO,QAAQ,aAAa;AACjC,gBAAQ,MAAM,SAAS,SAAO;AAC5B,cAAI,KAAK;AACP,iBAAK,OAAO,KACV,qBAAqB,iBAAiB,IAAI;AAAA;AAG9C;AAAA;AAAA;AAAA,OAGH;AAAA;AAAA,EAQC,wBAAwB,UAA4B;AAC5D,QAAI,KAAK,kBAAkB;AACzB,YAAM,CAAC,YAAW,OAAM,UAAS,UAAS;AAC1C,aAAOR,yBAAK,KAAK,eAAe,YAAW,OAAM,OAAM,GAAG;AAAA;AAE5D,UAAM,CAAC,WAAW,MAAM,SAAS,SAAS;AAC1C,WAAOA,yBAAK,KACV,eACA,UAAU,eACV,KAAK,eACL,KAAK,eACL,GAAG;AAAA;AAAA;;AC1PT,MAAM,iBAAiB,CAAC,WAA+C;AACrE,SAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,QAAI;AACF,YAAM,SAAgB;AACtB,aAAO,GAAG,QAAQ,WAAS,OAAO,KAAK;AACvC,aAAO,GAAG,SAAS;AACnB,aAAO,GAAG,OAAO,MAAM,QAAQ,OAAO,OAAO;AAAA,aACtC,GAAP;AACA,YAAM,IAAIS,sBAAe,qCAAqC;AAAA;AAAA;AAAA;AAKpE,MAAM,iBAAiB,CAAC,WAA6B;AACnD,QAAMuB,WAAS,IAAIC;AACnB,WAAO,KAAK;AACZ,WAAO,KAAK;AACZ,SAAOD;AAAA;4BAGmD;AAAA,EAK1D,YAAY,SAIT;AACD,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,gBAAgB,QAAQ;AAC7B,SAAK,SAAS,QAAQ;AAAA;AAAA,SAGjB,WAAW,QAAgB,QAA+B;AAC/D,QAAI,gBAAgB;AACpB,QAAI;AACF,sBAAgB,OAAO,UACrB;AAAA,aAEK,OAAP;AACA,YAAM,IAAI,MACR;AAAA;AAKJ,UAAM,uBAAuB,OAAO,UAClC;AAGF,UAAM,gBAAgB,IAAIE,8BAAY;AAAA,MACpC,cAAc,qBAAqB,UAAU;AAAA,MAC7C,eAAe,qBAAqB,UAAU;AAAA,MAC9C,cAAc,qBAAqB,UAAU;AAAA,MAC7C,QAAQ,qBAAqB,UAAU;AAAA;AAGzC,WAAO,IAAI,sBAAsB,EAAE,eAAe,eAAe;AAAA;AAAA,QAO7D,eAA2C;AAC/C,QAAI;AACF,YAAM,YAAY,MAAM,KAAK,cAAc,qBACzC,KAAK;AAGP,UAAI,uBAAuBC,iBAAW;AACpC,aAAK,OAAO,KACV,2DAA2D,KAAK;AAElE,eAAO;AAAA,UACL,aAAa;AAAA;AAAA;AAGjB,WAAK,OAAO,MACV,mEAAmE,KAAK;AAK1E,aAAO;AAAA,QACL,aAAa;AAAA;AAAA,aAER,KAAP;AACA,yBAAY;AACZ,WAAK,OAAO,MAAM,kCAAkC,IAAI;AACxD,aAAO;AAAA,QACL,aAAa;AAAA;AAAA;AAAA;AAAA,QASb,QAAQ;AAAA,IACZ;AAAA,IACA;AAAA,KAC2C;AAC3C,QAAI;AACF,YAAM,UAAoB;AAI1B,YAAM,mBAAmB,MAAM,uBAAuB;AACtD,YAAM,UAAUjC,kCAAc;AAC9B,YAAM,iBAA0C;AAChD,iBAAW,YAAY,kBAAkB;AAIvC,cAAM,mBAAmBF,yBAAK,SAAS,WAAW;AAIlD,cAAM,wBAAwB,iBAC3B,MAAMA,yBAAK,KACX,KAAKA,yBAAK,MAAM;AAGnB,cAAM,gBAAgB,GAAG,OAAO,SAAS,aAAa,OAAO,QAAQ,OAAO,SAAS;AACrF,cAAM,cAAc,GAAG,iBAAiB;AACxC,gBAAQ,KAAK;AAGb,cAAM,aAAa,QAAQ,YAAY;AACrC,gBAAM,aAAa,MAAMQ,uBAAG,SAAS;AACrC,gBAAM,SAAS,eAAe;AAC9B,iBAAO,KAAK,cAAc,OACxB,KAAK,eACL,aACA;AAAA;AAGJ,uBAAe,KAAK;AAAA;AAEtB,YAAM,QAAQ,IAAI;AAClB,WAAK,OAAO,KACV,4DAA4D,OAAO,SAAS,gCAAgC,iBAAiB;AAE/H,aAAO,EAAE;AAAA,aACF,GAAP;AACA,YAAM,eAAe,gDAAgD;AACrE,WAAK,OAAO,MAAM;AAClB,YAAM,IAAI,MAAM;AAAA;AAAA;AAAA,QAId,sBACJ,YAC2B;AAC3B,WAAO,MAAM,IAAI,QAA0B,OAAO,SAAS,WAAW;AACpE,YAAM,gBAAgB,GAAG,WAAW,aAAa,WAAW,QAAQ,WAAW;AAE/E,YAAM,mBAAmB,MAAM,KAAK,cAAc,SAChD,KAAK,eACL,GAAG;AAGL,UAAI,8BAA8B2B,iBAAW;AAC3C,cAAM,SAAS,iBAAiB;AAChC,YAAI;AACF,gBAAM,uBAAuB,MAAM,eAAe;AAClD,cAAI,CAAC,sBAAsB;AACzB,kBAAM,IAAI,MACR,8CAA8C;AAAA;AAIlD,gBAAM,mBAAmBd,0BAAM,MAC7B,qBAAqB,SAAS;AAGhC,kBAAQ;AAAA,iBACD,KAAP;AACA,6BAAY;AACZ,eAAK,OAAO,MAAM,IAAI;AACtB,iBAAO,IAAI,MAAM,IAAI;AAAA;AAAA,aAElB;AACL,eAAO;AAAA,UACL,SAAS,qDAAqD;AAAA;AAAA;AAAA;AAAA;AAAA,EAStE,aAA8B;AAC5B,WAAO,OAAO,KAAK,QAAQ;AAGzB,YAAM,WAAW,UAAU,IAAI,KAAK,QAAQ,OAAO;AAGnD,YAAM,gBAAgBrB,yBAAK,QAAQ;AACnC,YAAM,kBAAkB,2BAA2B;AAEnD,YAAM,mBAAmB,MAAM,KAAK,cAAc,SAChD,KAAK,eACL;AAGF,UAAI,8BAA8BmC,iBAAW;AAC3C,cAAM,SAAS,iBAAiB;AAEhC,YAAI;AAEF,qBAAW,CAAC,WAAW,gBAAgB,OAAO,QAC5C,kBACC;AACD,gBAAI,UAAU,WAAW;AAAA;AAG3B,cAAI,KAAK,MAAM,eAAe;AAAA,iBACvB,KAAP;AACA,6BAAY;AACZ,eAAK,OAAO,KACV,0EAA0E,KAAK,yBAAyB,aAAa,IAAI;AAE3H,cAAI,OAAO,KAAK,KAAK;AAAA;AAAA,aAElB;AACL,aAAK,OAAO,KACV,0EAA0E,KAAK,yBAAyB;AAE1G,YAAI,OAAO,KAAK,KAAK;AAAA;AAAA;AAAA;AAAA,QASrB,qBAAqB,QAAkC;AAC3D,UAAM,gBAAgB,GAAG,OAAO,SAAS,aAAa,OAAO,QAAQ,OAAO,SAAS;AACrF,QAAI;AACF,YAAM,eAAe,MAAM,KAAK,cAAc,YAC5C,KAAK,eACL,GAAG;AAGL,UAAI,0BAA0BA,iBAAW;AACvC,eAAO;AAAA;AAET,aAAO;AAAA,aACA,KAAP;AACA,yBAAY;AACZ,WAAK,OAAO,KAAK,IAAI;AACrB,aAAO;AAAA;AAAA;AAAA,QAIL,gBAAgB;AAAA,IACpB,iBAAiB;AAAA,IACjB,cAAc;AAAA,KACE;AAEhB,UAAM,aAAa,MAAM,KAAK;AAC9B,UAAM,UAAUjC,kCAAc;AAC9B,UAAM,QAAQ,IACZ,WAAW,IAAI,OACb,QAAQ,OAAM,SAAQ;AACpB,UAAI;AACJ,UAAI;AACF,kBAAU,oCAAoC;AAAA,eACvC,GAAP;AACA,2BAAY;AACZ,aAAK,OAAO,KAAK,EAAE;AACnB;AAAA;AAIF,UAAI,SAAS,SAAS;AACpB;AAAA;AAGF,UAAI;AACF,aAAK,OAAO,QAAQ,aAAa,WAAW;AAC5C,cAAM,KAAK,cAAc,KACvB,KAAK,eACL,MACA,KAAK,eACL;AAEF,YAAI,gBAAgB;AAClB,gBAAM,KAAK,cAAc,OAAO,KAAK,eAAe;AAAA;AAAA,eAE/C,GAAP;AACA,2BAAY;AACZ,aAAK,OAAO,KAAK,qBAAqB,SAAS,EAAE;AAAA;AAAA,OAElD;AAAA;AAAA,QAQO,2BACd,EAAE,WAAW,EAAE,QAAQ,MACJ;AACnB,QAAI,UAAoB;AACxB,UAAM,gBAAgB,KAAK,IAAI,GAAG,MAAM;AAExC,UAAM,aAAa,MAAM,KAAK,cAAc,KAC1C,KAAK,eACL,QACA;AAEF,cAAU,WAAW,IAAI,CAAC,WAAgB,OAAO;AAEjD,WAAO;AAAA;AAAA;;gBC9UY;AAAA,eAMR,WACX,QACA,EAAE,QAAQ,aACc;AAtC5B;AAuCI,UAAM,gBAAiB,aAAO,kBAC5B,+BADqB,YAElB;AAEL,YAAQ;AAAA,WACD;AACH,eAAO,KAAK;AACZ,eAAO,iBAAiB,WAAW,QAAQ;AAAA,WACxC;AACH,eAAO,KAAK;AACZ,eAAO,aAAa,WAAW,QAAQ;AAAA,WACpC;AACH,eAAO,KACL;AAEF,eAAO,wBAAwB,WAAW,QAAQ;AAAA,WAC/C;AACH,eAAO,KACL;AAEF,eAAO,sBAAsB,WAAW,QAAQ;AAAA,WAC7C;AACH,eAAO,KAAK;AACZ,eAAO,aAAa,WAAW,QAAQ,QAAQ;AAAA;AAE/C,eAAO,KAAK;AACZ,eAAO,aAAa,WAAW,QAAQ,QAAQ;AAAA;AAAA;AAAA;;;;;;;;;;;;;"}